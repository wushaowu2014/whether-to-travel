{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 代码中，n开头的为不去旅游的，q开头的为去旅游的，test开头的为测试集用户；\n",
    "#数字1对应表1，例如n1表示不去旅游中的表1，q2表示去旅游中的表2；\n",
    "#f1表示对表1提取特征的函数，f2表示对表2提取特征的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "C199525CFE7C47DD89EBD4744DCF2794",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzlt\n",
      "201708n  201708q  weather_data_2017\n",
      "201708n1.txt  201708n3.txt  201708n6.txt\n",
      "201708n2.txt  201708n4.txt  201708n7.txt\n",
      "201708q1.txt  201708q3.txt  201708q6.txt\n",
      "201708q2.txt  201708q4.txt  201708q7.txt\n",
      "201808\tweather_data_2018\n",
      "2018_1.txt  2018_2.txt\t2018_3.txt  2018_4.txt\t2018_6.txt  2018_7.txt\n",
      "time: 4.74 s\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录\n",
    "!ls /home/kesci/input/\n",
    "!ls /home/kesci/input/gzlt/train_set/\n",
    "!ls /home/kesci/input/gzlt/train_set/201708n/\n",
    "!ls /home/kesci/input/gzlt/train_set/201708q/\n",
    "!ls /home/kesci/input/gzlt/test_set/\n",
    "!ls /home/kesci/input/gzlt/test_set/201808/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "D2025EC9479C47D782144AE47CC47FEC",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kesci_submit\t  n_tr.csv\t    q_tr_quchong.csv  te_quchong.csv\n",
      "lost+found\t  n_tr_quchong.csv  te.csv\t      test66.csv\n",
      "mysubmission.csv  q66.csv\t    te_itft.csv       tr_itft.csv\n",
      "n66.csv\t\t  q_tr.csv\t    te_itft_rd.csv    tr_itft_rd.csv\n",
      "time: 790 ms\n"
     ]
    }
   ],
   "source": [
    "# 查看个人持久化工作区文件\n",
    "!ls /home/kesci/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "68095E5A8A754AC081ECCD7488E17B9A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 457 µs\n"
     ]
    }
   ],
   "source": [
    "# 查看当前kernerl下的package\n",
    "#!pip list --format=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "7DEE463228B243239D2C621748893D7E",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The klab-autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext klab-autotime\n",
      "time: 8.63 ms\n"
     ]
    }
   ],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "A6E903D21C474AC2AE9BB00DDF7A2D45",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.02 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "B2E29DB0063A41C4BD037C05D3A1D73B",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.19 ms\n"
     ]
    }
   ],
   "source": [
    "##定义读取训练集数据函数：\n",
    "def r_d(f):\n",
    "    n1= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s1.txt\"%(f,f), sep='\\t',header=None)\n",
    "    n1.columns=['账期','id','出账收入','labe']\n",
    "    n2= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s2.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n2.shape,'************\\n',n2[:5])\n",
    "    n2.columns=['id','手机品牌','终端型号','首次使用时间','末次使用时间','labe']\n",
    "    n3= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s3.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n3.shape,'************\\n',n3[:5])\n",
    "    n3.columns=['账期','id','联络圈规模','是否出省','是否出境','labe']\n",
    "    n4= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s4.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n4.shape,'************\\n',n4[:5])\n",
    "    n4.columns=['账期','id','漫出省份','labe']\n",
    "    #n5= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s5.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n5.shape,'************\\n',n5[:5])\n",
    "    #n5.columns=['账期','id','是否去过黔东南目标景区','labe']\n",
    "    n6= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s6.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n6.shape,'************\\n',n6[:5])\n",
    "    n6.columns=['日期','时段','id','经度','纬度','labe']\n",
    "    if f=='n':\n",
    "        n7= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s7.txt\"%(f,f), sep='\\t',header=None)\n",
    "        #print(n7.shape,'************\\n',n7[:5])\n",
    "        n7.columns=['账期','id','APP名称','流量']\n",
    "    else:\n",
    "        n7= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s7.txt\"%(f,f), sep='\\t',header=None)\n",
    "        #print(n7.shape,'************\\n',n7[:5])\n",
    "        n7.columns=['账期','id','APP名称','流量','labe']   \n",
    "    uid=pd.DataFrame(list(set(list(n1['id'])+list(n2['id'])+list(n3['id'])+list(n4['id'])+\\\n",
    "        list(n6['id'])+list(n7['id']))),columns=['id'])\n",
    "    return n1,n2,n3,n4,n6,n7,uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "BA0967817D3240E28960FEA32F76655A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50200, 1)\n",
      "time: 20 s\n"
     ]
    }
   ],
   "source": [
    "'''测试集直接读取：'''\n",
    "test1= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_1.txt\", sep='\\t',header=None)\n",
    "#print(test1.shape,'************\\n',test1[:5])\n",
    "test1.columns=['账期','id','出账收入']\n",
    "test2= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_2.txt\", sep='\\t',header=None)\n",
    "#print(test2.shape,'************\\n',test2[:5])\n",
    "test2.columns=['id','手机品牌','终端型号','首次使用时间','末次使用时间']\n",
    "test3= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_3.txt\", sep='\\t',header=None)\n",
    "#print(test3.shape,'************\\n',test3[:5])\n",
    "test3.columns=['账期','id','联络圈规模','是否出省','是否出境']\n",
    "test4= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_4.txt\", sep='\\t',header=None)\n",
    "#print(test4.shape,'************\\n',test4[:5])\n",
    "test4.columns=['账期','id','漫出省份']\n",
    "#test5= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_5.txt\", sep='\\t',header=None)\n",
    "#print(test5.shape,'************\\n',test5[:5])\n",
    "#test5.columns=['账期','id','是否去过黔东南目标景区']\n",
    "test6= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_6.txt\", sep='\\t',header=None)\n",
    "#print(test6.shape,'************\\n',test6[:5])\n",
    "test6.columns=['日期','时段','id','经度','纬度']\n",
    "test7= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_7.txt\", sep='\\t',header=None)\n",
    "#print(test7.shape,'************\\n',test7[:5])\n",
    "test7.columns=['账期','id','APP名称','流量']\n",
    "test_uid=pd.DataFrame(list(set(list(test1['id'])+list(test2['id'])+list(test3['id'])+list(test4['id'])+\\\n",
    "        list(test6['id'])+list(test7['id']))),columns=['id'])\n",
    "print(test_uid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "97BCA18B341445B68C7748A66E54921C",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.79 ms\n"
     ]
    }
   ],
   "source": [
    "##表1特征提取：\n",
    "def f1(d):\n",
    "    '''每月的出账收入'''\n",
    "    d=d.groupby(['id','账期'],as_index=False)['出账收入'].agg({'出账收入':'mean'})\n",
    "    riqi=list(set(d['账期']))\n",
    "    nd=pd.DataFrame(list(set(d['id'])),columns=['id'])\n",
    "    for i in riqi:\n",
    "        m=d[d['账期']==i][['id','出账收入']]\n",
    "        m.columns=['id',str(i)[-1]+'出账收入']\n",
    "        nd=pd.merge(nd,m,how='left',on=['id'])\n",
    "    del riqi,m,d\n",
    "    return nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "35D844404068403C854A81A6F6760954",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93400\n",
      "time: 44.2 s\n"
     ]
    }
   ],
   "source": [
    "##读取训练数据：\n",
    "n1,n2,n3,n4,n6,n7,n_uid=r_d(f='n')\n",
    "q1,q2,q3,q4,q6,q7,q_uid=r_d('q')\n",
    "print(len(n_uid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "05A35B2813BE4A248DBF76B4E65D8006",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289203, 7) \n",
      "******************\n",
      " id          5600\n",
      "手机品牌         750\n",
      "终端型号        5290\n",
      "首次使用时间    285263\n",
      "末次使用时间    277300\n",
      "labe           1\n",
      "y              4\n",
      "dtype: int64\n",
      "time: 241 ms\n"
     ]
    }
   ],
   "source": [
    "##表2提取特征：\n",
    "import time\n",
    "def f2(d):\n",
    "    d=d.fillna(-99)\n",
    "    #nd=pd.DataFrame(list(set(d['id'])),columns=['id'])\n",
    "    m=d.groupby(['id'],as_index=False)['id'].agg({'id_count_2':'count'})\n",
    "    m1=d[d['手机品牌']==-99].groupby(['id'],as_index=False)['手机品牌'].agg({'手机品牌_2':'count'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])#.fillna(0)\n",
    "    m['手机品牌缺失率_2']=m['手机品牌_2']/m['id_count_2']\n",
    "    \n",
    "    m1=d[d['手机品牌']!=-99].groupby(['id'],as_index=False)['手机品牌'].agg({'手机品牌nunique_2':'nunique'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    \n",
    "    m1=d[d['终端型号']==-99].groupby(['id'],as_index=False)['终端型号'].agg({'终端型号count_2':'count'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    m['终端型缺失率_2']=m['终端型号count_2']/m['id_count_2']\n",
    "    \n",
    "    m1=d[d['终端型号']!=-99].groupby(['id'],as_index=False)['终端型号'].agg({'终端型号nunique_2':'nunique'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    \n",
    "    d['末次使用时间']=d['末次使用时间'].apply(lambda x:\\\n",
    "    int(time.mktime(time.strptime(str(int(x)), \"%Y%m%d%H%M%S\"))) if int(x)>0 else -99)\n",
    "    d['首次使用时间']=d['首次使用时间'].apply(lambda x:\\\n",
    "    int(time.mktime(time.strptime(str(int(x)), \"%Y%m%d%H%M%S\"))) if int(x)>0 else -99)\n",
    "    d['末-首次使用时间']=(d['末次使用时间']-d['首次使用时间'])/3600\n",
    "    \n",
    "    m1=d.groupby(['id'],as_index=False)['末-首次使用时间'].agg({'末-首次使用时间mean':'mean',\\\n",
    "        '末-首次使用时间max':'max',\\\n",
    "        '末-首次使用时间min':'min',\\\n",
    "        '末-首次使用时间std':'std',\\\n",
    "         })\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    del m1\n",
    "    ##加小时，周期特征：\n",
    "    d['末次month']=d['末次使用时间'].apply(lambda x:\\\n",
    "    int(str(x)[4:6]) if int(x)>0 else -99)\n",
    "    d['首次month']=d['首次使用时间'].apply(lambda x:\\\n",
    "    int(str(x)[4:6]) if int(x)>0 else -99)\n",
    "    m1=d[d['末次month']!=-99].groupby(['id'],as_index=False)['末次month'].agg({'末次monthmean':'mean',\\\n",
    "        '末次month间max':'max',\\\n",
    "        '末次monthmin':'min',\\\n",
    "        '末次monthstd':'std',\\\n",
    "         })\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    m1=d[d['首次month']!=-99].groupby(['id'],as_index=False)['首次month'].agg({'首次monthmean':'mean',\\\n",
    "        '末次month间max':'max',\\\n",
    "        '末次monthmin':'min',\\\n",
    "        '首次monthstd':'std',\\\n",
    "         })\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    \n",
    "    m1=d[d['末次month']==-99].groupby(['id'],as_index=False)['末次month'].agg({'末次month99':'count'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    m1=d[d['首次month']==-99].groupby(['id'],as_index=False)['首次month'].agg({'首次month99':'count'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    \n",
    "    del m1\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "5D753510E808446C81834728C9A15862",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.53 ms\n"
     ]
    }
   ],
   "source": [
    "##表3提取特征：\n",
    "def f3(d):\n",
    "    m=pd.DataFrame(list(set(d['id'])),columns=['id'])\n",
    "    m1=d[d['联络圈规模']!=-1].groupby(['id'],as_index=False)['联络圈规模'].agg({'联络圈规模_3':'mean'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id']).fillna(-1)\n",
    "    \n",
    "    m1=d.groupby(['id','账期'],as_index=False)['是否出省'].agg({'是否出省':'mean'})\n",
    "    for i in list(set(m1['账期'])):\n",
    "        mm=m1[m1['账期']==i][['id','是否出省']]\n",
    "        mm.columns=['id',str(i)[-1]+'是否出省']\n",
    "        m=pd.merge(m,mm,how='left',on=['id'])\n",
    "    m1=d.groupby(['id','账期'],as_index=False)['是否出境'].agg({'是否出境':'mean'})\n",
    "    for i in list(set(m1['账期'])):\n",
    "        mm=m1[m1['账期']==i][['id','是否出境']]\n",
    "        mm.columns=['id',str(i)[-1]+'是否出境']\n",
    "        m=pd.merge(m,mm,how='left',on=['id'])\n",
    "    del m1\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "3DEAD94B13EC4822BD0E1A12F5CEB936",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36499, 4) \n",
      "******************\n",
      " 账期          2\n",
      "id      12958\n",
      "漫出省份       31\n",
      "labe        1\n",
      "dtype: int64\n",
      "          账期                id 漫出省份  labe\n",
      "4306  201706  9987406611703100   广西     1\n",
      "4305  201706  9987406611703100   广西     1\n",
      "4304  201706  9987406611703100   广西     1\n",
      "4307  201706  9987406611703100   广西     1\n",
      "4308  201706  9987406611703100   广西     1\n",
      "4309  201706  9987406611703100   广东     1\n",
      "4303  201706  9987406611703100   广东     1\n",
      "4302  201706  9987406611703100   广东     1\n",
      "3048  201707  9986868023697813   浙江     1\n",
      "3052  201707  9986868023697813   江西     1\n",
      "[nan, '西藏', '陕西', '江苏', '四川', '北京', '天津', '吉林', '青海', '河北', '辽宁', '广西', '新疆', '宁夏', '甘肃', '重庆', '广东', '山西', '山东', '黑龙江', '安徽', '浙江', '河南', '内蒙古', '福建', '贵州', '海南', '湖南', '湖北', '上海', '江西', '云南']\n",
      "(400, 4)\n",
      "time: 32.7 ms\n"
     ]
    }
   ],
   "source": [
    "##表4提取特征：\n",
    "def f4(d):\n",
    "    m=d.groupby(['id'],as_index=False)['漫出省份'].agg({'漫出省份count_4':'count',\\\n",
    "       '漫出省份nunique_4':'nunique'\n",
    "    })\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "CDB241C25B6F4F3082CB04F0D49586FE",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.42 ms\n"
     ]
    }
   ],
   "source": [
    "##表5提取特征：\n",
    "def f5(d):\n",
    "    m=pd.DataFrame(list(set(d['id'])),columns=['id'])\n",
    "    for month in list(set(d['账期'])):\n",
    "        m1=d[d['账期']==month][['id','是否去过黔东南目标景区']]\n",
    "        m1.columns=['id',str(month)[-1]+'是否去过黔东南目标景区']\n",
    "        m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    del m1\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "69D552E7AC344AA5843BC82A09B398AE",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 日期   时段                id          经度         纬度  labe  \\\n",
      "2309855  2017-06-13  0.0  5885140327545400  106.737292  26.491528     1   \n",
      "137727   2017-07-16  0.0  7628503026181759  106.689060  26.577640     1   \n",
      "695596   2017-07-26  0.0  9795428722591227  106.681080  26.571250     1   \n",
      "1432309  2017-07-16  0.0  5806773608286300  107.780970  26.196025     1   \n",
      "2047252  2017-06-22  0.0   891223810895000  106.693860  26.591010     1   \n",
      "1189054  2017-07-27  0.0  3850423310904900  106.690790  26.562120     1   \n",
      "695600   2017-07-22  0.0  2474772103093852  106.718840  26.587360     1   \n",
      "1189058  2017-07-07  0.0  4797083896419762  106.614929  26.688263     1   \n",
      "435693   2017-07-20  0.0  4256790410176798  105.490640  26.202920     1   \n",
      "435692   2017-07-19  0.0  3758073107952600  106.724600  26.616190     1   \n",
      "2597625  2017-06-12  0.0  3846048361566500  106.623568  26.652063     1   \n",
      "2047260  2017-06-10  0.0  9650368071572749  106.665640  26.586700     1   \n",
      "2597603  2017-06-18  0.0  1101839616642900  106.728910  26.570340     1   \n",
      "2047269  2017-06-08  0.0  7245296504232500  106.667140  26.501350     1   \n",
      "2597595  2017-06-22  0.0  1747937196312800  107.909510  26.914700     1   \n",
      "1189069  2017-07-09  0.0  5145744579898000  107.790410  26.201990     1   \n",
      "695607   2017-07-23  0.0  5178589918530881  106.626411  26.655279     1   \n",
      "435683   2017-07-23  0.0  2504979731189300  106.655830  26.572298     1   \n",
      "137745   2017-07-21  0.0  6903325618365443  106.699510  26.530820     1   \n",
      "2597580  2017-06-14  0.0  8993711591962700  106.953847  26.462630     1   \n",
      "1432320  2017-07-19  0.0  7946941104735400  106.735490  26.624130     1   \n",
      "913559   2017-07-02  0.0  4897687526410600  106.661484  26.665812     1   \n",
      "2047234  2017-06-22  0.0  7737833109464800  106.670370  26.443870     1   \n",
      "2244463  2017-06-17  0.0  4000052220846900  106.032230  27.029210     1   \n",
      "1189042  2017-07-14  0.0  3695436684576081  107.998060  26.568060     1   \n",
      "435736   2017-07-27  0.0  9650368071572749  106.665560  26.586670     1   \n",
      "2415561  2017-06-19  0.0  9185183283887520  106.710000  26.576390     1   \n",
      "236234   2017-07-20  0.0  7020959965090950  106.645167  26.621859     1   \n",
      "1567914  2017-07-17  0.0  5887057730731840  106.772780  26.646940     1   \n",
      "695574   2017-07-12  0.0  8387559449016737  106.758582  26.612195     1   \n",
      "...             ...  ...               ...         ...        ...   ...   \n",
      "2047344  2017-06-24  0.0  6079645484753000  106.619392  26.629684     1   \n",
      "2415597  2017-06-20  0.0  9738702287593060  106.692882  26.525711     1   \n",
      "695626   2017-07-31  0.0  9583304489139871  106.679200  26.585420     1   \n",
      "2244432  2017-06-06  0.0  2946954444356004  107.728900  27.001700     1   \n",
      "2415592  2017-06-27  0.0  1242788107513823  106.710830  26.582780     1   \n",
      "2597553  2017-06-23  0.0  7570292829422700  106.722580  26.565970     1   \n",
      "435673   2017-07-02  0.0  9409803602728276  109.196260  27.734730     1   \n",
      "2047299  2017-06-16  0.0  2946954444356004  106.757949  26.539728     1   \n",
      "435670   2017-07-14  0.0  8110372154692300  106.095019  25.170247     1   \n",
      "1189086  2017-07-17  0.0  5277010708654808  108.146110  26.497500     1   \n",
      "2597542  2017-06-14  0.0  7534995232174000  106.632965  26.654030     1   \n",
      "1567834  2017-07-06  0.0  6615948711678674  106.685830  26.522220     1   \n",
      "913545   2017-07-20  0.0  1504536471578200  106.688423  26.735189     1   \n",
      "1313701  2017-07-18  0.0  3119429690740068  106.690230  26.573730     1   \n",
      "137764   2017-07-24  0.0  7831559506749097  106.620560  26.606400     1   \n",
      "913544   2017-07-30  0.0  7286039507140400  106.772540  26.647010     1   \n",
      "2597533  2017-06-04  0.0  8739746634163100  106.696390  26.588060     1   \n",
      "2597529  2017-06-22  0.0  4618866476474800  106.695570  26.521460     1   \n",
      "695620   2017-07-29  0.0  6522231850464129  106.696670  26.523890     1   \n",
      "913542   2017-07-03  0.0  7286039507140400  106.772540  26.647010     1   \n",
      "2047311  2017-06-24  0.0  2290281731779600  106.734980  26.493360     1   \n",
      "2047316  2017-06-06  0.0  6294352313434600  106.693390  26.504240     1   \n",
      "695623   2017-07-04  0.0  9583304489139871  106.687640  26.575190     1   \n",
      "2597519  2017-06-27  0.0  7585085013759000  106.724440  26.569850     1   \n",
      "137775   2017-07-15  0.0  3304108204572609  106.771462  26.594041     1   \n",
      "2415589  2017-06-30  0.0  7009265394845802  106.655830  26.572298     1   \n",
      "1567786  2017-07-14  0.0  7333777731509505  106.684660  26.564240     1   \n",
      "1189013  2017-07-16  0.0  6742150986071000  106.716390  26.586110     1   \n",
      "435741   2017-07-07  0.0  7245296504232500  106.666080  26.494010     1   \n",
      "695521   2017-07-09  0.0  6424754068186366  106.689170  26.530280     1   \n",
      "\n",
      "                时间戳  \n",
      "2309855  1497312000  \n",
      "137727   1500163200  \n",
      "695596   1501027200  \n",
      "1432309  1500163200  \n",
      "2047252  1498089600  \n",
      "1189054  1501113600  \n",
      "695600   1500681600  \n",
      "1189058  1499385600  \n",
      "435693   1500508800  \n",
      "435692   1500422400  \n",
      "2597625  1497225600  \n",
      "2047260  1497052800  \n",
      "2597603  1497744000  \n",
      "2047269  1496880000  \n",
      "2597595  1498089600  \n",
      "1189069  1499558400  \n",
      "695607   1500768000  \n",
      "435683   1500768000  \n",
      "137745   1500595200  \n",
      "2597580  1497398400  \n",
      "1432320  1500422400  \n",
      "913559   1498953600  \n",
      "2047234  1498089600  \n",
      "2244463  1497657600  \n",
      "1189042  1499990400  \n",
      "435736   1501113600  \n",
      "2415561  1497830400  \n",
      "236234   1500508800  \n",
      "1567914  1500249600  \n",
      "695574   1499817600  \n",
      "...             ...  \n",
      "2047344  1498262400  \n",
      "2415597  1497916800  \n",
      "695626   1501459200  \n",
      "2244432  1496707200  \n",
      "2415592  1498521600  \n",
      "2597553  1498176000  \n",
      "435673   1498953600  \n",
      "2047299  1497571200  \n",
      "435670   1499990400  \n",
      "1189086  1500249600  \n",
      "2597542  1497398400  \n",
      "1567834  1499299200  \n",
      "913545   1500508800  \n",
      "1313701  1500336000  \n",
      "137764   1500854400  \n",
      "913544   1501372800  \n",
      "2597533  1496534400  \n",
      "2597529  1498089600  \n",
      "695620   1501286400  \n",
      "913542   1499040000  \n",
      "2047311  1498262400  \n",
      "2047316  1496707200  \n",
      "695623   1499126400  \n",
      "2597519  1498521600  \n",
      "137775   1500076800  \n",
      "2415589  1498780800  \n",
      "1567786  1499990400  \n",
      "1189013  1500163200  \n",
      "435741   1499385600  \n",
      "695521   1499558400  \n",
      "\n",
      "[100 rows x 7 columns]\n",
      "time: 596 ms\n"
     ]
    }
   ],
   "source": [
    "##表6提取特征：\n",
    "def f6_1(d):\n",
    "    d['时间戳']=d['日期'].apply(lambda x:\\\n",
    "    int(time.mktime(time.strptime(x, \"%Y-%m-%d\"))))\n",
    "    d['日期_m']=d['日期'].apply(lambda x:int(str(x).split('-')[1]))\n",
    "    m=d.groupby(['id'],as_index=False)['日期'].agg({\\\n",
    "       '日期nunique_6':'nunique',\\\n",
    "       '日期count_6':'count'\n",
    "    })\n",
    "    m['日期count_6/日期nunique_6']=m['日期count_6']/(1+m['日期nunique_6'])\n",
    "    m=m.drop(['日期count_6'],axis=1)\n",
    "    \n",
    "    m1=d.groupby(['id','日期_m'],as_index=False)['日期_m'].agg({\\\n",
    "       '日期_m_count':'count'\n",
    "    })\n",
    "    for i in list(set(m1['日期_m'])):\n",
    "        m2=m1[m1['日期_m']==i][['id','日期_m_count']]\n",
    "        m2.columns=['id','日期_m_count'+str(i)]\n",
    "        m=m.merge(m2,how='left',on=['id']).fillna(0)\n",
    "        m['日期_m_count'+str(i)]=m['日期_m_count'+str(i)].apply(lambda x:1 if x>0 else 0)\n",
    "    del m1,m2\n",
    "    return m\n",
    "    \n",
    "def f6(d):\n",
    "    d['基站']=d['经度'].astype(str)+'_'+d['纬度'].astype(str)\n",
    "    m=d.groupby(['id'],as_index=False)['基站'].agg({'基站count_6':'count',\\\n",
    "       '基站nunique_6':'nunique'\n",
    "    })\n",
    "    m1=d.groupby(['id'],as_index=False)['经度'].agg({'经度mean_6':'mean',\\\n",
    "       '经度max_6':'max',\\\n",
    "       '经度min_6':'min',\\\n",
    "       '经度std_6':'std',\\\n",
    "       '经度median_6':'median',\n",
    "       '经度nunique_6':'nunique',\n",
    "    })\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    del m1\n",
    "    m1=d.groupby(['id'],as_index=False)['纬度'].agg({'纬度mean_6':'mean',\\\n",
    "       '纬度max_6':'max',\\\n",
    "       '纬度min_6':'min',\\\n",
    "       '纬度std_6':'std',\\\n",
    "       '纬度median_6':'median',\n",
    "       '纬度nunique_6':'nunique',\n",
    "    })\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    del m1\n",
    "    m1=d.groupby(['id'],as_index=False)['时段'].agg({'时段mean_6':'mean',\\\n",
    "       '时段max_6':'max',\\\n",
    "       '时段min_6':'min',\\\n",
    "       '时段std_6':'std',\\\n",
    "       '时段median_6':'median',\n",
    "       '时段nunique_6':'nunique',\n",
    "    })\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    del m1\n",
    "    \n",
    "    t=[]\n",
    "    for u in m['id']:\n",
    "        m1=d[d.id==u][['基站','经度','纬度','日期','时段']]\n",
    "        m1=m1.sort_values(by=['日期','时段'],ascending=True).reset_index(drop=True)\n",
    "        inds=[]\n",
    "        if len(m1)>1:\n",
    "            for i in range(len(m1)-1):\n",
    "                if m1.loc[i,'基站']==m1.loc[i+1,'基站']:\n",
    "                    if m1.loc[i,'日期']==m1.loc[i+1,'日期']:\n",
    "                        inds.append(i+1)\n",
    "            m1=m1.drop(inds,axis=0).reset_index(drop=True)\n",
    "            #print(len(m1))\n",
    "        \n",
    "        if len(m1)>0:\n",
    "            t.append([u,m1['经度'].max(),m1['经度'].min(),m1['经度'].mean(),m1['经度'].std(),\\\n",
    "            m1['纬度'].max(),m1['纬度'].min(),m1['纬度'].mean(),m1['纬度'].std(),\\\n",
    "            len(m1)])\n",
    "        #else:\n",
    "        #    t.append([u,0,0,0,0])\n",
    "               \n",
    "    t=pd.DataFrame(t)\n",
    "    t.columns=['id']+['fff{}'.format(i) for i in range(t.shape[1]-1)]\n",
    "    del m1\n",
    "    m=pd.merge(m,t,how='left',on=['id'])\n",
    "    del t\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "8D9850C2B0324BBEA6352600E2F3C393",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173117, 5) \n",
      "******************\n",
      " 账期           2\n",
      "id        4717\n",
      "APP名称      762\n",
      "流量       11485\n",
      "labe         1\n",
      "dtype: int64\n",
      "            账期              id       APP名称      流量  labe\n",
      "88662   201707   1448103998000        乐视视频    0.00     1\n",
      "53088   201707   1448103998000        最美天气    0.01     1\n",
      "23842   201707   1448103998000     360手机卫士    0.03     1\n",
      "90240   201707   1448103998000       爱奇艺视频    0.00     1\n",
      "115177  201706   1448103998000          QQ    0.00     1\n",
      "154063  201706   1448103998000      腾讯手机管家    0.00     1\n",
      "74423   201707   1448103998000          微信    0.09     1\n",
      "8813    201707   1448103998000        优酷视频    0.00     1\n",
      "56182   201707   1448103998000          QQ    0.01     1\n",
      "72515   201707   1448103998000       开心消消乐    0.00     1\n",
      "126719  201706  17398718813730          QQ   10.01     1\n",
      "9004    201707  17398718813730        美图秀秀    0.67     1\n",
      "71785   201707  17398718813730        百度地图    1.80     1\n",
      "12634   201707  17398718813730   掌阅iReader    0.00     1\n",
      "101515  201706  17398718813730        QQ邮箱    0.02     1\n",
      "101516  201706  17398718813730  WPS Office    0.00     1\n",
      "141839  201706  17398718813730       车轮查违章    0.00     1\n",
      "140153  201706  17398718813730      腾讯手机管家    0.00     1\n",
      "27974   201707  17398718813730        途牛旅游    0.19     1\n",
      "158267  201706  17398718813730       去哪儿旅行    0.47     1\n",
      "86468   201707  17398718813730       爱奇艺视频    0.34     1\n",
      "2953    201707  17398718813730      腾讯手机管家    0.00     1\n",
      "62493   201707  17398718813730        滴滴出行    0.78     1\n",
      "144206  201706  17398718813730      腾讯充值中心    0.00     1\n",
      "75384   201707  17398718813730   nearme云笔记    0.00     1\n",
      "155611  201706  17398718813730        手机淘宝    4.08     1\n",
      "32563   201707  17398718813730      QQ同步助手    0.90     1\n",
      "51086   201707  17398718813730        手机淘宝   13.66     1\n",
      "24793   201707  17398718813730     360手机卫士    3.49     1\n",
      "24415   201707  17398718813730    OPPO软件商店    0.01     1\n",
      "...        ...             ...         ...     ...   ...\n",
      "145231  201706  17398718813730        航旅枞横    0.03     1\n",
      "172916  201706  17398718813730          美团    1.15     1\n",
      "123276  201706  17398718813730        腾讯视频    0.00     1\n",
      "149701  201706  17398718813730       搜狗输入法    0.00     1\n",
      "46522   201707  17398718813730       讯飞输入法    0.00     1\n",
      "39107   201707  17398718813730         唯品会    1.02     1\n",
      "76376   201707  17398718813730  WPS Office    0.68     1\n",
      "64757   201707  17398718813730          QQ   14.10     1\n",
      "53606   201707  68156596675520       宝宝树孕育    0.02     1\n",
      "10491   201707  68156596675520        美团外卖   10.37     1\n",
      "52842   201707  68156596675520        考拉FM    0.00     1\n",
      "21845   201707  68156596675520      百度手机助手    0.00     1\n",
      "82181   201707  68156596675520        乐视视频    0.01     1\n",
      "23135   201707  68156596675520        乐视体育    2.33     1\n",
      "110330  201706  68156596675520        QQ音乐    1.84     1\n",
      "13056   201707  68156596675520        百度地图    2.48     1\n",
      "44455   201707  68156596675520          美团    4.80     1\n",
      "146050  201706  68156596675520      腾讯充值中心    0.00     1\n",
      "102359  201706  68156596675520          微信  341.18     1\n",
      "146386  201706  68156596675520      QQ同步助手    0.01     1\n",
      "69680   201707  68156596675520    AppStore   10.11     1\n",
      "53410   201707  68156596675520       美味不用等    0.14     1\n",
      "35935   201707  68156596675520        苏宁易购    0.00     1\n",
      "152637  201706  68156596675520          快手    0.00     1\n",
      "154751  201706  68156596675520        美图秀秀    0.00     1\n",
      "20628   201707  68156596675520        新浪微博    0.73     1\n",
      "99317   201706  68156596675520      球探体育比分    0.00     1\n",
      "98561   201706  68156596675520          美团    0.23     1\n",
      "66275   201707  68156596675520        优酷视频    3.28     1\n",
      "1453    201707  68156596675520        58同城    3.77     1\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "##表7提取特征：\n",
    "print(q7.sort_values(by=['id'],ascending=True)[:100])\n",
    "def f7(d):\n",
    "    m=d.groupby(['id'],as_index=False)['APP名称'].agg({'APP名称count_7':'count',\\\n",
    "       'APP名称nunique_7':'nunique'\n",
    "    })\n",
    "    m1=d.groupby(['id'],as_index=False)['流量'].agg({'流量mean_7':'mean',\\\n",
    "       '流量sum_7':'sum',\\\n",
    "       '流量max_7':'max',\\\n",
    "       '流量min_7':'min',\\\n",
    "       '流量std_7':'std',\\\n",
    "    })\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    del m1\n",
    "    return m\n",
    "def f7_1(d):\n",
    "    d['账期_month']=d['账期'].apply(lambda x:str(x)[-2:])\n",
    "    mm=pd.DataFrame(list(set(d['id'])),columns=['id'])\n",
    "    m=d.groupby(['id','账期_month'],as_index=False)['APP名称'].agg({'APP名称count_7':'count'})\n",
    "    for  col in list(set(m['账期_month'])):\n",
    "        guo=m[m['账期_month']==col][['id','APP名称count_7']]\n",
    "        guo.columns=['id','APP名称count_7'+col]\n",
    "        mm=mm.merge(guo,how='left',on=['id']).fillna(0)\n",
    "        mm['APP名称count_7'+col]=mm['APP名称count_7'+col].apply(lambda x:1 if x>0 else 0)\n",
    "    \n",
    "    m=d.groupby(['id','账期_month'],as_index=False)['流量'].agg({'流量_mean_7':'mean'})\n",
    "    for  col in list(set(m['账期_month'])):\n",
    "        guo=m[m['账期_month']==col][['id','流量_mean_7']]\n",
    "        guo.columns=['id','流量_mean_7'+col]\n",
    "        mm=mm.merge(guo,how='left',on=['id'])#.fillna(0)\n",
    "    del m,guo\n",
    "    return mm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "D200A9C19224480C82EBFE962902DA95",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5600, 78) (93400, 78) (50200, 77)\n",
      "time: 23min 4s\n"
     ]
    }
   ],
   "source": [
    "##结合特征：q开头的为去旅游的训练集，n开头的为不去旅游的训练集，test开头的为测试集\n",
    "q_uid['label']=1 ##训练集中去旅游的用户\n",
    "n_uid['label']=0 ##训练集中不去旅游的用户\n",
    "q_train=pd.merge(q_uid,f1(q1),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f2(q2),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f3(q3),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f4(q4),how='left',on=['id'])\n",
    "#q_train=pd.merge(q_train,f5(q5),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f6_1(q6),how='left',on=['id'])\n",
    "try:\n",
    "    mm=pd.read_csv('q66.csv')\n",
    "except:\n",
    "    mm=f6(q6) ##表6特征提取花时间，保存方便再次使用\n",
    "    mm.to_csv('q66.csv',index=None)\n",
    "q_train=pd.merge(q_train,mm,how='left',on=['id'])\n",
    "\n",
    "q_train=pd.merge(q_train,f7(q7),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f7_1(q7),how='left',on=['id'])\n",
    "\n",
    "n_train=pd.merge(n_uid,f1(n1),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f2(n2),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f3(n3),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f4(n4),how='left',on=['id'])\n",
    "#n_train=pd.merge(n_train,f5(n5),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f6_1(n6),how='left',on=['id'])\n",
    "try:\n",
    "    mm=pd.read_csv('n66.csv')\n",
    "except:\n",
    "    mm=f6(n6)\n",
    "    mm.to_csv('n66.csv',index=None)\n",
    "n_train=pd.merge(n_train,mm,how='left',on=['id'])\n",
    "#n_train=pd.merge(n_train,f6(n6),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f7(n7),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f7_1(n7),how='left',on=['id'])\n",
    "\n",
    "test=pd.merge(test_uid,f1(test1),how='left',on=['id'])\n",
    "test=pd.merge(test,f2(test2),how='left',on=['id'])\n",
    "test=pd.merge(test,f3(test3),how='left',on=['id'])\n",
    "test=pd.merge(test,f4(test4),how='left',on=['id'])\n",
    "#test=pd.merge(test,f5(test5),how='left',on=['id'])\n",
    "test=pd.merge(test,f6_1(test6),how='left',on=['id'])\n",
    "try:\n",
    "    mm=pd.read_csv('test66.csv')\n",
    "except:\n",
    "    mm=f6(test6)\n",
    "    mm.to_csv('test66.csv',index=None)\n",
    "test=pd.merge(test,mm,how='left',on=['id'])\n",
    "#test=pd.merge(test,f6(test6),how='left',on=['id'])\n",
    "test=pd.merge(test,f7(test7),how='left',on=['id'])\n",
    "test=pd.merge(test,f7_1(test7),how='left',on=['id'])\n",
    "\n",
    "print(q_train.shape,n_train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "D9DC88C54D8447BA8F8F276A4CE249BF",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    93390\n",
      "1     5599\n",
      "Name: label, dtype: int64\n",
      "0    93400\n",
      "1     5600\n",
      "Name: label, dtype: int64\n",
      "1009387204000 9999717474837900\n",
      "训练集中大于m的个数 98989\n",
      "训练集且去旅游中大于m的个数 5599\n",
      "10204443360000\n",
      "595941207920 9999920327264900\n",
      "测试集中小于m的个数 5\n",
      "相同用户个数： 0\n",
      "0\n",
      "相同用户标签分布： Series([], Name: label, dtype: int64)\n",
      "Index(['id', 'label', '6出账收入', '7出账收入', 'id_count_2', '手机品牌_2', '手机品牌缺失率_2',\n",
      "       '手机品牌nunique_2', '终端型号count_2', '终端型缺失率_2', '终端型号nunique_2',\n",
      "       '末-首次使用时间mean', '末-首次使用时间max', '末-首次使用时间min', '末-首次使用时间std',\n",
      "       '末次monthmean', '末次month间max_x', '末次monthmin_x', '末次monthstd',\n",
      "       '首次monthmean', '末次month间max_y', '末次monthmin_y', '首次monthstd',\n",
      "       '末次month99', '首次month99', '联络圈规模_3', '6是否出省', '7是否出省', '6是否出境', '7是否出境',\n",
      "       '联络圈规模_3_1', '漫出省份count_4', '漫出省份nunique_4', '日期nunique_6',\n",
      "       '日期count_6/日期nunique_6', '日期_m_count5', '日期_m_count6', '日期_m_count7',\n",
      "       '基站count_6', '基站nunique_6', '经度mean_6', '经度max_6', '经度min_6', '经度std_6',\n",
      "       '经度median_6', '经度nunique_6', '纬度mean_6', '纬度max_6', '纬度min_6',\n",
      "       '纬度std_6', '纬度median_6', '纬度nunique_6', '时段mean_6', '时段max_6',\n",
      "       '时段min_6', '时段std_6', '时段median_6', '时段nunique_6', 'fff0', 'fff1',\n",
      "       'fff2', 'fff3', 'fff4', 'fff5', 'fff6', 'fff7', 'fff8', 'APP名称count_7',\n",
      "       'APP名称nunique_7', '流量mean_7', '流量sum_7', '流量max_7', '流量min_7',\n",
      "       '流量std_7', 'APP名称count_707', 'APP名称count_706', '流量_mean_707',\n",
      "       '流量_mean_706'],\n",
      "      dtype='object')\n",
      "Index(['id', '6出账收入', '7出账收入', 'id_count_2', '手机品牌_2', '手机品牌缺失率_2',\n",
      "       '手机品牌nunique_2', '终端型号count_2', '终端型缺失率_2', '终端型号nunique_2',\n",
      "       '末-首次使用时间mean', '末-首次使用时间max', '末-首次使用时间min', '末-首次使用时间std',\n",
      "       '末次monthmean', '末次month间max_x', '末次monthmin_x', '末次monthstd',\n",
      "       '首次monthmean', '末次month间max_y', '末次monthmin_y', '首次monthstd',\n",
      "       '末次month99', '首次month99', '联络圈规模_3', '6是否出省', '7是否出省', '6是否出境', '7是否出境',\n",
      "       '联络圈规模_3_1', '漫出省份count_4', '漫出省份nunique_4', '日期nunique_6',\n",
      "       '日期count_6/日期nunique_6', '日期_m_count5', '日期_m_count6', '日期_m_count7',\n",
      "       '基站count_6', '基站nunique_6', '经度mean_6', '经度max_6', '经度min_6', '经度std_6',\n",
      "       '经度median_6', '经度nunique_6', '纬度mean_6', '纬度max_6', '纬度min_6',\n",
      "       '纬度std_6', '纬度median_6', '纬度nunique_6', '时段mean_6', '时段max_6',\n",
      "       '时段min_6', '时段std_6', '时段median_6', '时段nunique_6', 'fff0', 'fff1',\n",
      "       'fff2', 'fff3', 'fff4', 'fff5', 'fff6', 'fff7', 'fff8', 'APP名称count_7',\n",
      "       'APP名称nunique_7', '流量mean_7', '流量sum_7', '流量max_7', '流量min_7',\n",
      "       '流量std_7', 'APP名称count_707', 'APP名称count_706', '流量_mean_707',\n",
      "       '流量_mean_706'],\n",
      "      dtype='object')\n",
      "time: 3.58 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train=pd.concat([q_train,n_train],axis=0).reset_index(drop=True)\n",
    "\n",
    "#train=train[train['id']>10000000000000].reset_index(drop=True)\n",
    "print(train[train['id']>10000000000000]['label'].value_counts())\n",
    "label=train['label']\n",
    "print(train['label'].value_counts())\n",
    "print(train['id'].min(),train['id'].max())\n",
    "print('训练集中大于m的个数',len(train[train['id']>10000000000000]))\n",
    "print('训练集且去旅游中大于m的个数',len(q_train[q_train['id']>10000000000000]))\n",
    "print(train[train['id']>10000000000000]['id'].min())\n",
    "print(test['id'].min(),test['id'].max())\n",
    "print('测试集中小于m的个数',len(test[test['id']<10000000000000]))\n",
    "\n",
    "xiu=[i for i in test['id'].values if int(i) in train['id'].values]\n",
    "print('相同用户个数：',len(xiu))\n",
    "xiu2=train[train.id.isin(xiu)][['id','label']].reset_index(drop=True)\n",
    "print(len(xiu2))\n",
    "print('相同用户标签分布：',xiu2['label'].value_counts())\n",
    "#import matplotlib.pyplot as plt\n",
    "print(train.columns)\n",
    "print(test.columns)\n",
    "nn=test[['id']]\n",
    "nn['label']=-1\n",
    "dd=pd.concat([train[train['id']>10000000000000][['id','label']],nn[['id','label']]],axis=0).reset_index(drop=True)\n",
    "dd=dd.sort_values(by='id')\n",
    "#print(dd[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "7D243194F85141FF9FBA2AD8E918547A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.09 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "def xgb_model(new_train,y,new_test,lr,N):\n",
    "  '''定义模型'''\n",
    "  xgb_params = {'booster': 'gbtree',\n",
    "          'eta':lr, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "          'objective':'binary:logistic',\n",
    "          'eval_metric': 'auc',\n",
    "          'silent': True,\n",
    "          }\n",
    "  #skf=StratifiedKFold(y,n_folds=5,shuffle=True,random_state=2018)\n",
    "  skf=StratifiedKFold(n_splits=N,shuffle=True,random_state=42)\n",
    "  oof_xgb=np.zeros(new_train.shape[0])\n",
    "  prediction_xgb=np.zeros(new_test.shape[0])\n",
    "  for i,(tr,va) in enumerate(skf.split(new_train,y)):\n",
    "    print('fold:',i+1,'training')\n",
    "    dtrain = xgb.DMatrix(new_train[tr],y[tr])\n",
    "    dvalid = xgb.DMatrix(new_train[va],y[va])\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid_data')]\n",
    "    bst = xgb.train(dtrain=dtrain, num_boost_round=30000, evals=watchlist, early_stopping_rounds=200, \\\n",
    "    verbose_eval=400, params=xgb_params)\n",
    "    oof_xgb[va] += bst.predict(xgb.DMatrix(new_train[va]), ntree_limit=bst.best_ntree_limit)\n",
    "    prediction_xgb += bst.predict(xgb.DMatrix(new_test), ntree_limit=bst.best_ntree_limit)\n",
    "  print('the roc_auc_score for train:',roc_auc_score(y,oof_xgb))\n",
    "  prediction_xgb/=N\n",
    "  return oof_xgb,prediction_xgb\n",
    "def lgb_model(new_train,y,new_test):\n",
    "    params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'num_leaves': 1000,\n",
    "    'verbose': -1,\n",
    "    'max_depth': -1,\n",
    "  #  'reg_alpha':2.2,\n",
    "  #  'reg_lambda':1.4,\n",
    "    'seed':42,\n",
    "    }\n",
    "    #skf=StratifiedKFold(y,n_folds=5,shuffle=True,random_state=2018)\n",
    "    skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "    oof_lgb=np.zeros(new_train.shape[0])\n",
    "    prediction_lgb=np.zeros(new_test.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i,(tr,va) in enumerate(skf.split(new_train,y)):\n",
    "        print('fold:',i+1,'training')\n",
    "        dtrain = lgb.Dataset(new_train[tr],y[tr])\n",
    "        dvalid = lgb.Dataset(new_train[va],y[va],reference=dtrain)\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=30000, valid_sets=dvalid, verbose_eval=400,early_stopping_rounds=200)\n",
    "        oof_lgb[va] += bst.predict(new_train[va], num_iteration=bst.best_iteration)\n",
    "        prediction_lgb += bst.predict(new_test, num_iteration=bst.best_iteration)\n",
    "        '''\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = list(new_train.columns)\n",
    "        fold_importance_df[\"importance\"] = bst.feature_importance(importance_type='split', iteration=bst.best_iteration)\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        '''\n",
    "    \n",
    "    print('the roc_auc_score for train:',roc_auc_score(y,oof_lgb))\n",
    "    prediction_lgb/=5\n",
    "    return oof_lgb,prediction_lgb,feature_importance_df\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def model(train_,label,test_):\n",
    "    #模型训练：\n",
    "    n_folds=5\n",
    "    stack_train = np.zeros(train_.shape[0])\n",
    "    stack_test = np.zeros(test_.shape[0])\n",
    "    skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=2019)\n",
    "    for i, (tr, va) in enumerate(skf.split(train_,label)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "        #cfc = LinearSVC(random_state=2019)\n",
    "        cfc= LogisticRegression(random_state=2019,C=8)\n",
    "        cfc.fit(train_[tr], label[tr])\n",
    "        #score_va = cfc._predict_proba_lr(train_[va])[:,1]\n",
    "        #score_te = cfc._predict_proba_lr(test_)[:,1]\n",
    "        score_va = cfc.predict_proba(train_[va])[:,1]\n",
    "        score_te = cfc.predict_proba(test_)[:,1]\n",
    "        print(\"per model roc_auc_score:\", metrics.roc_auc_score(label[va],score_va))\n",
    "        stack_train[va] += score_va\n",
    "        stack_test += score_te\n",
    "    print(\"model roc_auc_score:\",roc_auc_score(label,stack_train))\n",
    "    stack_test /= n_folds\n",
    "    return stack_train,stack_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "F23C982AD503407C9AAF0B77D0618EAB",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 60.1 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "new_tr=train.drop(['label'],axis=1)\n",
    "new_te=test[new_tr.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量部分从这里开始..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "0C5AE0F2510746348D019E88174C03D2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.61 s\n"
     ]
    }
   ],
   "source": [
    "##按时间，构造每个用户的基站序列：基站直接由经纬度表示\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "def one_hot_col(col):\n",
    "    '''标签编码'''\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(col)\n",
    "    return lbl\n",
    "gc.collect()\n",
    "'''\n",
    "try:\n",
    "    n_tr=pd.read_csv('n_tr.csv') ##训练集中不去旅游用户的基站序列\n",
    "    q_tr=pd.read_csv('q_tr.csv') ##训练集中去旅游用户的基站序列\n",
    "    te=pd.read_csv('te.csv') ##测试集中用户的基站序列\n",
    "except:\n",
    "    n6=n6[['id','基站']]\n",
    "    q6=q6[['id','基站']]\n",
    "    test6=test6[['id','基站']]\n",
    "\n",
    "    seq=list(set(list(set(n6['基站']))+list(set(q6['基站']))+list(set(test6['基站'])))) ##训练集和测试集中所有基站总个数\n",
    "    print('总基站个数：',len(seq))\n",
    "    \n",
    "    ##对基站做变换：\n",
    "    mm={}\n",
    "    for i in range(len(seq)):\n",
    "        mm[seq[i]]=i\n",
    "    ##对训练集，测试集中的基站进行映射：\n",
    "    n6['基站']=n6['基站'].map(mm)\n",
    "    q6['基站']=q6['基站'].map(mm)\n",
    "    test6['基站']=test6['基站'].map(mm)\n",
    "\n",
    "    ##映射后的训练集中不去旅游的序列：\n",
    "    m={}\n",
    "    for i,row in n6.iterrows():\n",
    "        m.setdefault(row['id'],[]).append(row['基站'])\n",
    "    for k, v in m.items():\n",
    "        m[k]=' '.join([str(i) for i in v])\n",
    "    m=pd.DataFrame.from_dict(m,orient='index')\n",
    "    m=m.reset_index()\n",
    "    m.columns=['id','基站']\n",
    "    m.to_csv('n_tr.csv',index=None)\n",
    "    print('n_tr训练集构造完毕！')\n",
    "    \n",
    "    ##训练集去旅游的序列：\n",
    "    m={}\n",
    "    for i,row in q6.iterrows():\n",
    "        m.setdefault(row['id'],[]).append(row['基站'])\n",
    "    for k, v in m.items():\n",
    "        m[k]=' '.join([str(i) for i in v])\n",
    "    m=pd.DataFrame.from_dict(m,orient='index')\n",
    "    m=m.reset_index()\n",
    "    m.columns=['id','基站']\n",
    "    m.to_csv('q_tr.csv',index=None)\n",
    "    print('q_tr训练集构造完毕！')\n",
    "    \n",
    "    ##测试集的序列：\n",
    "    gc.collect()\n",
    "    m={}\n",
    "    for i,row in test6.iterrows():\n",
    "        m.setdefault(row['id'],[]).append(row['基站'])\n",
    "    for k, v in m.items():\n",
    "        m[k]=' '.join([str(i) for i in v])\n",
    "    m=pd.DataFrame.from_dict(m,orient='index')\n",
    "    m=m.reset_index()\n",
    "    m.columns=['id','基站']\n",
    "    m.to_csv('te.csv',index=None)\n",
    "    print('te测试集构造完毕！')\n",
    "    del m\n",
    "try:\n",
    "    n_tr=pd.read_csv('n_tr.csv')\n",
    "    q_tr=pd.read_csv('q_tr.csv')\n",
    "    te=pd.read_csv('te.csv')\n",
    "except:\n",
    "    print('数据不存在！')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "C5D8B61A6E1747B38868DB29CD323273",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.09 s\n"
     ]
    }
   ],
   "source": [
    "##和上面的一模一样，构造用户基站序列。这里根据日期，对一天中，连续两个相同的基站去重。（直接跑就行，不用管）\n",
    "from tqdm import *\n",
    "'''\n",
    "try:\n",
    "    n_tr_quchong=pd.read_csv('n_tr_quchong.csv')\n",
    "    q_tr_quchong=pd.read_csv('q_tr_quchong.csv')\n",
    "    te_quchong=pd.read_csv('te_quchong.csv')\n",
    "except:\n",
    "    n1,n2,n3,n4,n6,n7,n_uid=r_d(f='n')\n",
    "    q1,q2,q3,q4,q6,q7,q_uid=r_d('q')\n",
    "    test6= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_6.txt\", sep='\\t',header=None)\n",
    "    test6.columns=['日期','时段','id','经度','纬度']\n",
    "\n",
    "    n6['基站']=n6['经度'].astype(str)+'_'+n6['纬度'].astype(str)\n",
    "    q6['基站']=q6['经度'].astype(str)+'_'+q6['纬度'].astype(str)\n",
    "    test6['基站']=test6['经度'].astype(str)+'_'+test6['纬度'].astype(str)\n",
    "    n6=n6[['id','基站','日期']]\n",
    "    q6=q6[['id','基站','日期']]\n",
    "    test6=test6[['id','基站','日期']]\n",
    "    \n",
    "    ##先对数据集去重：\n",
    "    delindex=[]\n",
    "    for i in tqdm(range(len(n6)-1)):\n",
    "        if n6.loc[i,'id']==n6.loc[i+1,'id']:\n",
    "            if n6.loc[i,'日期']==n6.loc[i+1,'日期'] and n6.loc[i,'基站']==n6.loc[i+1,'基站']:\n",
    "                delindex.append(i)\n",
    "    n6_quchong=n6.drop(delindex,axis=0).reset_index(drop=True)\n",
    "    \n",
    "    delindex=[]\n",
    "    for i in tqdm(range(len(q6)-1)):\n",
    "        if q6.loc[i,'id']==q6.loc[i+1,'id']:\n",
    "            if q6.loc[i,'日期']==q6.loc[i+1,'日期'] and q6.loc[i,'基站']==q6.loc[i+1,'基站']:\n",
    "                delindex.append(i)\n",
    "    q6_quchong=q6.drop(delindex,axis=0).reset_index(drop=True)\n",
    "    \n",
    "    delindex=[]\n",
    "    for i in tqdm(range(len(test6)-1)):\n",
    "        if test6.loc[i,'id']==test6.loc[i+1,'id']:\n",
    "            if test6.loc[i,'日期']==test6.loc[i+1,'日期'] and test6.loc[i,'基站']==test6.loc[i+1,'基站']:\n",
    "                delindex.append(i)\n",
    "    test6_quchong=test6.drop(delindex,axis=0).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    seq=list(set(list(set(n6_quchong['基站']))+list(set(q6_quchong['基站']))+list(set(test6_quchong['基站']))))\n",
    "    print('总基站个数：',len(seq))\n",
    "    mm={}\n",
    "    for i in range(len(seq)):\n",
    "        mm[seq[i]]=i\n",
    "    ##对基站进行映射：\n",
    "    n6_quchong['基站']=n6_quchong['基站'].map(mm)\n",
    "    q6_quchong['基站']=q6_quchong['基站'].map(mm)\n",
    "    test6_quchong['基站']=test6_quchong['基站'].map(mm)\n",
    "\n",
    "    ##训练集不去旅游的序列：\n",
    "    \n",
    "    m={}\n",
    "    for i,row in n6_quchong.iterrows():\n",
    "        m.setdefault(row['id'],[]).append(row['基站'])\n",
    "    for k, v in m.items():\n",
    "        m[k]=' '.join([str(i) for i in v])\n",
    "    m=pd.DataFrame.from_dict(m,orient='index')\n",
    "    m=m.reset_index()\n",
    "    m.columns=['id','基站']\n",
    "    m.to_csv('n_tr_quchong.csv',index=None)\n",
    "    print('n_tr_quchong训练集构造完毕！')\n",
    "    \n",
    "    ##训练集去旅游的序列：\n",
    "    m={}\n",
    "    for i,row in q6_quchong.iterrows():\n",
    "        m.setdefault(row['id'],[]).append(row['基站'])\n",
    "    for k, v in m.items():\n",
    "        m[k]=' '.join([str(i) for i in v])\n",
    "    m=pd.DataFrame.from_dict(m,orient='index')\n",
    "    m=m.reset_index()\n",
    "    m.columns=['id','基站']\n",
    "    m.to_csv('q_tr_quchong.csv',index=None)\n",
    "    print('q_tr_quchong训练集构造完毕！')\n",
    "    \n",
    "    ##测试集的序列：\n",
    "    gc.collect()\n",
    "    m={}\n",
    "    for i,row in test6_quchong.iterrows():\n",
    "        m.setdefault(row['id'],[]).append(row['基站'])\n",
    "    for k, v in m.items():\n",
    "        m[k]=' '.join([str(i) for i in v])\n",
    "    m=pd.DataFrame.from_dict(m,orient='index')\n",
    "    m=m.reset_index()\n",
    "    m.columns=['id','基站']\n",
    "    m.to_csv('te_quchong.csv',index=None)\n",
    "    print('te_quchong测试集构造完毕！')\n",
    "    del m\n",
    "try:\n",
    "    n_tr_quchong=pd.read_csv('n_tr_quchong.csv')\n",
    "    q_tr_quchong=pd.read_csv('q_tr_quchong.csv')\n",
    "    te_quchong=pd.read_csv('te_quchong.csv')\n",
    "except:\n",
    "    print('数据不存在！')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "D46BD32BB8014CC6AA8086D7B636DE7D",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76346, 2) (5575, 2) (41848, 2)\n",
      "time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "##用TfidfVectorizer训练模型：基站去重部分\n",
    "print(n_tr_quchong.shape,q_tr_quchong.shape,te_quchong.shape)\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "try:\n",
    "    te_itft_rd=pd.read_csv('te_itft_rd.csv')\n",
    "    tr_itft_rd=pd.read_csv('tr_itft_rd.csv')\n",
    "except:\n",
    "    gc.collect()\n",
    "    tr=pd.concat([n_tr_quchong,q_tr_quchong,te_quchong],axis=0).reset_index(drop=True)\n",
    "\n",
    "    vec = TfidfVectorizer(ngram_range=(1,1),min_df=1, max_df=0.9,use_idf=1,smooth_idf=1,\\\n",
    "                      sublinear_tf=1)\n",
    "\n",
    "    term_doc = vec.fit_transform(tr['基站'])\n",
    "    trn_term_doc =term_doc[:len(n_tr_quchong)+len(q_tr_quchong)]\n",
    "    test_term_doc =term_doc[len(n_tr_quchong)+len(q_tr_quchong):]\n",
    "    del term_doc\n",
    "    print(trn_term_doc.shape,test_term_doc.shape)\n",
    "    \n",
    "    y=pd.Series([0]*n_tr_quchong.shape[0]+[1]*q_tr_quchong.shape[0])\n",
    "\n",
    "    oof_xgb,prediction_xgb=xgb_model(trn_term_doc,pd.Series(y),test_term_doc,0.01,5)\n",
    "    ##保存特征：\n",
    "    gc.collect()\n",
    "    te_quchong=te_quchong.drop(['基站'],axis=1)\n",
    "    te_quchong['itft_rd']=prediction_xgb\n",
    "    te_quchong.to_csv('te_itft_rd.csv',index=None)\n",
    "    del te_quchong\n",
    "    gc.collect()\n",
    "    tr=tr.drop(['基站'],axis=1)\n",
    "    tr=tr[:len(n_tr)+len(q_tr)]\n",
    "    tr['itft_rd']=oof_xgb\n",
    "    tr.to_csv('tr_itft_rd.csv',index=None)\n",
    "    del tr\n",
    "    gc.collect()\n",
    "    #oof_lgb,prediction_lgb,feature_importance_df=lgb_model(trn_term_doc,pd.Series(y),test_term_doc)\n",
    "try:\n",
    "    te_itft_rd=pd.read_csv('te_itft_rd.csv')\n",
    "    tr_itft_rd=pd.read_csv('tr_itft_rd.csv')\n",
    "except:\n",
    "    print('shujubucunzai!')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "AE454478D0CA4C288D0001597F60F1E3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 147 ms\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "##用TfidfVectorizer训练模型：基站不去重部分\n",
    "try:\n",
    "    te_itft=pd.read_csv('te_itft.csv')\n",
    "    tr_itft=pd.read_csv('tr_itft.csv')\n",
    "except:\n",
    "    gc.collect()\n",
    "    tr=pd.concat([n_tr,q_tr,te],axis=0).reset_index(drop=True)\n",
    "\n",
    "    vec = TfidfVectorizer(ngram_range=(1,1),min_df=1, max_df=0.9,use_idf=1,smooth_idf=1,\\\n",
    "                      sublinear_tf=1)\n",
    "\n",
    "    term_doc = vec.fit_transform(tr['基站'])\n",
    "    trn_term_doc =term_doc[:len(n_tr)+len(q_tr)]\n",
    "    test_term_doc =term_doc[len(n_tr)+len(q_tr):]\n",
    "    del term_doc\n",
    "    print(trn_term_doc.shape,test_term_doc.shape)\n",
    "    \n",
    "    y=[0]*n_tr.shape[0]+[1]*q_tr.shape[0]\n",
    "    oof_xgb,prediction_xgb=xgb_model(trn_term_doc,pd.Series(y),test_term_doc,0.01,5)\n",
    "    \n",
    "    ##保存特征：\n",
    "    gc.collect()\n",
    "    te=te.drop(['基站'],axis=1)\n",
    "    te['itft_xgb']=prediction_xgb\n",
    "    te.to_csv('te_itft.csv',index=None)\n",
    "    del te\n",
    "    gc.collect()\n",
    "    tr=tr.drop(['基站'],axis=1)\n",
    "    tr=tr[:len(n_tr)+len(q_tr)]\n",
    "    tr['itft_xgb']=oof_xgb\n",
    "    tr.to_csv('tr_itft.csv',index=None)\n",
    "    del tr\n",
    "    gc.collect()\n",
    "\n",
    "try:\n",
    "    te_itft=pd.read_csv('te_itft.csv')\n",
    "    tr_itft=pd.read_csv('tr_itft.csv')\n",
    "except:\n",
    "    print('shujubucunzai!')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "1043F574DDE14F42845EA463A6346AB4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50200, 78)\n",
      "time: 248 ms\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "##结合词向量特征：\n",
    "print(pd.merge(new_te,te_itft,how='left',on=['id']).shape)\n",
    "##结合itft特征（基站没去重）：\n",
    "new_tr=pd.merge(new_tr,tr_itft,how='left',on=['id'])\n",
    "new_te=pd.merge(new_te,te_itft,how='left',on=['id'])\n",
    "##结合itft特征（基站去重）：\n",
    "new_tr=pd.merge(new_tr,tr_itft_rd,how='left',on=['id'])\n",
    "new_te=pd.merge(new_te,te_itft_rd,how='left',on=['id'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "2A3286B45C7144ABA3C35C6C65720008",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 training\n",
      "[0]\ttrain-auc:0.99045\tvalid_data-auc:0.987603\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[400]\ttrain-auc:0.998736\tvalid_data-auc:0.996656\n",
      "[800]\ttrain-auc:0.999837\tvalid_data-auc:0.998267\n",
      "Stopping. Best iteration:\n",
      "[876]\ttrain-auc:0.99989\tvalid_data-auc:0.998288\n",
      "\n",
      "fold: 2 training\n",
      "[0]\ttrain-auc:0.990545\tvalid_data-auc:0.986774\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[400]\ttrain-auc:0.998655\tvalid_data-auc:0.9963\n",
      "[800]\ttrain-auc:0.999864\tvalid_data-auc:0.998187\n",
      "[1200]\ttrain-auc:0.999986\tvalid_data-auc:0.998209\n",
      "Stopping. Best iteration:\n",
      "[1065]\ttrain-auc:0.99997\tvalid_data-auc:0.998235\n",
      "\n",
      "fold: 3 training\n",
      "[0]\ttrain-auc:0.989163\tvalid_data-auc:0.990338\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[400]\ttrain-auc:0.998682\tvalid_data-auc:0.997606\n",
      "[800]\ttrain-auc:0.999821\tvalid_data-auc:0.998274\n",
      "Stopping. Best iteration:\n",
      "[606]\ttrain-auc:0.999599\tvalid_data-auc:0.998317\n",
      "\n",
      "fold: 4 training\n",
      "[0]\ttrain-auc:0.988347\tvalid_data-auc:0.993955\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[400]\ttrain-auc:0.998699\tvalid_data-auc:0.998912\n",
      "[800]\ttrain-auc:0.999801\tvalid_data-auc:0.999164\n",
      "[1200]\ttrain-auc:0.999984\tvalid_data-auc:0.999182\n",
      "Stopping. Best iteration:\n",
      "[1004]\ttrain-auc:0.999936\tvalid_data-auc:0.999201\n",
      "\n",
      "fold: 5 training\n",
      "[0]\ttrain-auc:0.989799\tvalid_data-auc:0.98807\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[400]\ttrain-auc:0.998661\tvalid_data-auc:0.997235\n",
      "[800]\ttrain-auc:0.999829\tvalid_data-auc:0.998473\n",
      "Stopping. Best iteration:\n",
      "[956]\ttrain-auc:0.999929\tvalid_data-auc:0.998495\n",
      "\n",
      "the roc_auc_score for train: 0.9983614333524575\n",
      "confusion_matrix for lr:\n",
      " [[93373     2]\n",
      " [  235  5365]]\n",
      "classification_report for lr\n",
      ":               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     93375\n",
      "           1       1.00      0.96      0.98      5600\n",
      "\n",
      "    accuracy                           1.00     98975\n",
      "   macro avg       1.00      0.98      0.99     98975\n",
      "weighted avg       1.00      1.00      1.00     98975\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_B_info=my_xx1.describe()\\nmeaningful_col=[]\\nfor col in train_B_info.columns:\\n    if train_B_info.ix[0,col]>my_xx1.shape[0]*0.01:#ix是用了索引指定的数据，ix[0,col]为0行col列的数;shape[0]返回行数\\n        meaningful_col.append(col)#将col放到meaningful_col最后一列中\\nmy_xx1=my_xx1[meaningful_col].copy()#copy to train_B_1\\nmeaningful_col.remove('label')\\nmy_te1=my_te1[meaningful_col].copy()\\nprint(meaningful_col)\\n\\n#oof_xgb1,prediction_xgb1=xgb_model(np.array(my_xx1.drop(['id','label'],axis=1)),#                       my_xx1['label'].values,#                       np.array(my_te1.drop(['id'],axis=1)),lr=0.1)\\noof_xgb1,prediction_xgb1=xgb_model(np.array(my_xx[meaningful_col+['id']]),                       my_xx['label'].values,                       np.array(my_te[meaningful_col+['id']]),0.01)\\n#oof_xgb1,prediction_xgb1,impo=lgb_model(np.array(my_xx1.drop(['id','label'],axis=1)),#                       my_xx1['label'].values,#                       np.array(my_te1.drop(['id'],axis=1)))\\n\\n#result1=my_xx1[['id']]\\n#result1['label']=oof_xgb1\\nresult1=my_te1[['id']]\\nresult1['label']=prediction_xgb1\\n\\nfrom sklearn import metrics\\nfrom sklearn.metrics import confusion_matrix\\nr=[1 if i>0.6 else 0 for i in oof_xgb1]\\nprint('confusion_matrix for lr:\\n', metrics.confusion_matrix(my_xx1['label'].values,r))\\nprint('classification_report for lr\\n:', metrics.classification_report(my_xx1['label'].values,r))\\n\""
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23min 13s\n"
     ]
    }
   ],
   "source": [
    "##最终模型训练：\n",
    "my_xx=new_tr.copy() ##训练集\n",
    "my_xx['label']=label\n",
    "my_te=new_te.copy() ##测试集\n",
    "\n",
    "useful_id_tr4=list(set(q4['id']))+list(set(n4['id'])) ##训练集中，在表4的所有用户\n",
    "useful_id_tr7=list(set(n7['id'])) ##训练集中，在表7的不去旅游的用户\n",
    "useful_id_tr2=list(set(n2['id'])) ##训练集中，在表2的不去旅游的用户\n",
    "\n",
    "useful_id_tr=list(set(q6['id']))+list(set(n6['id']))##训练集中，在表6的所有用户\n",
    "#useful_id_tr=list(set(n6['id']))\n",
    "useful_id_te=list(set(test6['id']))##测试集中，在表6的所有用户\n",
    "useful_id_tr1=[i for i in list(set(n1['id'])) if i not in useful_id_tr2] ##训练集中，不在表2的，且不去旅游的用户：25个\n",
    "\n",
    "my_xx1=my_xx[(my_xx.id.isin(useful_id_tr))&\\  ##只取在表6的用户训练\n",
    "(~my_xx.id.isin(useful_id_tr1))  ##不去旅游中，去掉不再表2的25个用户\n",
    "].reset_index(drop=True)\n",
    "\n",
    "##模型训练：\n",
    "oof_xgb1,prediction_xgb1=xgb_model(np.array(my_xx1.drop(['id','label'],axis=1)),\\\n",
    "                       my_xx1['label'].values,\\\n",
    "                       np.array(my_te.drop(['id'],axis=1)),0.01,5)\n",
    "                       \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "r=[1 if i>0.6 else 0 for i in oof_xgb1]\n",
    "print('confusion_matrix for lr:\\n', metrics.confusion_matrix(my_xx1['label'].values,r))\n",
    "print('classification_report for lr\\n:', metrics.classification_report(my_xx1['label'].values,r))\n",
    "print('到这里就结束了！下面的cell可以不用管！下面保存特征：')\n",
    "stacking_tr=my_xx1[['id']]\n",
    "stacking_tr['pre']=oof_xgb1\n",
    "stacking_te=my_te[['id']]\n",
    "stacking_te['pre']=prediction_xgb1\n",
    "stacking_tr.to_csv('stacking_tr.csv',index=None) ##训练集stacking特征\n",
    "stacking_te.to_csv('stacking_te.csv',index=None) ##测试集stacking特征\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求分错的用户是哪些用户..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "E5EC2AFE7EC9472196DC423A7CEF3CCF",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix for lr:\n",
      " [[60882     0]\n",
      " [  136  3516]]\n",
      "classification_report for lr\n",
      ":               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     60882\n",
      "           1       1.00      0.96      0.98      3652\n",
      "\n",
      "    accuracy                           1.00     64534\n",
      "   macro avg       1.00      0.98      0.99     64534\n",
      "weighted avg       1.00      1.00      1.00     64534\n",
      "\n",
      "time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "r=[1 if i>0.5 else 0 for i in oof_xgb1]\n",
    "print('confusion_matrix for lr:\\n', metrics.confusion_matrix(my_xx1['label'],r))\n",
    "print('classification_report for lr\\n:', metrics.classification_report(my_xx1['label'],r))\n",
    "\n",
    "##分错的用户存于cuo_id中：字段含义为：用户id,真实标签label,模型预测概率：\n",
    "cuo_id=[]\n",
    "for i in range(len(r)):\n",
    "    if r[i]!=my_xx1['label'][i]:\n",
    "        cuo_id.append([my_xx1.loc[i,'id'],my_xx1['label'][i],oof_xgb1[i]])\n",
    "cuo_id=pd.DataFrame(cuo_id,columns=['id','label','pre'])\n",
    "print(cuo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "ACD51241CA67416497FE9B08183E9861",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id  label       pre\n",
      "0    1837472551960900      1  0.027038\n",
      "1    4209653910405500      1  0.044190\n",
      "2    1449939502334500      1  0.036933\n",
      "3    4396279590290000      1  0.054725\n",
      "4    7217048611554200      1  0.005852\n",
      "5    8724124477031600      1  0.151961\n",
      "6    1886006687304000      1  0.023341\n",
      "7    9447563464608100      1  0.040569\n",
      "8    1468333456263700      1  0.004428\n",
      "9    8537829126507120      1  0.103902\n",
      "10   4275873842793200      1  0.010385\n",
      "11   6413063328533255      1  0.007890\n",
      "12   8090377968520000      1  0.025474\n",
      "13   8605746201759424      1  0.009946\n",
      "14   6518101911965700      1  0.219370\n",
      "15   8024934942806133      1  0.034839\n",
      "16   2139805723627900      1  0.215362\n",
      "17   6497562401772428      1  0.054449\n",
      "18   4538334245033000      1  0.063827\n",
      "19   5260608249861300      1  0.006627\n",
      "20   1742729033422100      1  0.025594\n",
      "21   2613895006393700      1  0.096620\n",
      "22   8400462932678237      1  0.098320\n",
      "23   5175939607729779      1  0.171622\n",
      "24   2571335048926900      1  0.014296\n",
      "25   6453872145700901      1  0.074512\n",
      "26   8332302281382000      1  0.228022\n",
      "27   3246338611452160      1  0.037505\n",
      "28   9909243001510150      1  0.102325\n",
      "29   1422636026009900      1  0.064696\n",
      "..                ...    ...       ...\n",
      "106  5288726521470700      1  0.026590\n",
      "107  3742733483171600      1  0.010794\n",
      "108  8649770267009163      1  0.205843\n",
      "109  5907132378342100      1  0.008942\n",
      "110  3343721802883835      1  0.282161\n",
      "111  1777406824637200      1  0.018546\n",
      "112  3103114935591720      1  0.176689\n",
      "113  8334941865862200      1  0.017467\n",
      "114  6209654599542000      1  0.008027\n",
      "115  7481649770095900      1  0.039433\n",
      "116  7634547007088000      1  0.018708\n",
      "117  4338331738957200      1  0.008203\n",
      "118  4918929422052100      1  0.011104\n",
      "119  6684324349635400      1  0.007699\n",
      "120  9147184936742653      1  0.063966\n",
      "121  1182722765156100      1  0.005181\n",
      "122  9828854563565400      1  0.015919\n",
      "123  2084316722393200      1  0.026286\n",
      "124  1318154108039300      1  0.095542\n",
      "125  7805177084375400      1  0.004252\n",
      "126  1781620213084700      1  0.125962\n",
      "127  6533906134397700      1  0.017956\n",
      "128  6796522532009000      1  0.006789\n",
      "129  1855076285412795      1  0.050315\n",
      "130  5617608612345600      1  0.006097\n",
      "131  6117917522458800      1  0.010218\n",
      "132  2271125805725900      1  0.008111\n",
      "133  3914803358202200      1  0.169781\n",
      "134  8014605962083800      1  0.026988\n",
      "135  3895955937034080      1  0.015225\n",
      "\n",
      "[136 rows x 3 columns]\n",
      "not_in_q6_id: 25\n",
      "120\n",
      "33392\n",
      "5575\n",
      "19\n",
      "93400    5600\n",
      "dtype: int64\n",
      "                    id  juli\n",
      "0        1448103998000     1\n",
      "1       17398718813730     1\n",
      "2       61132623486000     1\n",
      "3       68156596675520     1\n",
      "4       76819334576430     1\n",
      "5       78745100940550     1\n",
      "6      110229638660000     1\n",
      "7      122134826301000     1\n",
      "8      132923269304000     1\n",
      "9      138204830829320     1\n",
      "10     151410167359000     1\n",
      "11     156240192035720     1\n",
      "12     175782119707000     1\n",
      "13     185575638460000     1\n",
      "14     248900960618000     1\n",
      "15     249899887513060     1\n",
      "16     258454242055110     1\n",
      "17     259300844410590     1\n",
      "18     272882210900000     1\n",
      "19     278567685381570     1\n",
      "20     298786714995120     1\n",
      "21     319872250262850     1\n",
      "22     378154746326000     1\n",
      "23     386201281826190     1\n",
      "24     392012661583710     1\n",
      "25     413049492192000     1\n",
      "26     423537057602000     1\n",
      "27     446225717902240     1\n",
      "28     457966001635000     1\n",
      "29     482443906579000     1\n",
      "...                ...   ...\n",
      "5570  9949306732442400     1\n",
      "5571  9951275478403940     1\n",
      "5572  9951784630500900     1\n",
      "5573  9952471046244371     1\n",
      "5574  9952942455174600     1\n",
      "5575  9953031403491763     1\n",
      "5576  9953367172303976     1\n",
      "5577  9961161786316866     1\n",
      "5578  9962336438704152     1\n",
      "5579  9964820533972420     1\n",
      "5580  9967562259798369     1\n",
      "5581  9968360920078988     1\n",
      "5582  9969183084341240     1\n",
      "5583  9970112164020124     1\n",
      "5584  9970944771369370     1\n",
      "5585  9971084551635000     1\n",
      "5586  9972449056973700     1\n",
      "5587  9975135305988664     1\n",
      "5588  9975551782691929     1\n",
      "5589  9976864706725900     1\n",
      "5590  9980665401813680     1\n",
      "5591  9984087129255571     1\n",
      "5592  9985634960133791     1\n",
      "5593  9985838559667198     1\n",
      "5594  9986868023697813     1\n",
      "5595  9987406611703100     1\n",
      "5596  9990783148099373     1\n",
      "5597  9990914144074667     1\n",
      "5598  9992915017489200     1\n",
      "5599  9997949290636466     1\n",
      "\n",
      "[5600 rows x 2 columns]\n",
      "(5600, 2)\n",
      "time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "##这里可以不用管：\n",
    "print(cuo_id)\n",
    "#print(new_tr[new_tr['id'].isin([int(i) for i in cuo_id['id']])])\n",
    "all_id=list(set(q1['id']))\n",
    "q6_id=list(set(q6['id']))\n",
    "not_in_q6_id=[i for i in all_id if i not in q6_id]\n",
    "print('not_in_q6_id:',len(not_in_q6_id))\n",
    "#print(q1.nunique())\n",
    "#print(cuo_id[:10])\n",
    "#print(q1['id'][:10])\n",
    "print(len(set(q7[q7['id'].isin([int(i) for i in cuo_id['id']])]['id'])))\n",
    "print(len(set(n6['id'].unique())&set(n7['id'].unique())))\n",
    "print(len(set(q6['id'].unique())&set(q1['id'].unique())))\n",
    "print(len(set(not_in_q6_id)&set(q4['id'].unique())))\n",
    "#print(q3[q3['id'].isin([int(i) for i in cuo_id['id']])].sort_values(by=['id'],ascending=True))\n",
    "#print(q2[:200])\n",
    "mmm=list(n3.sort_values(by=['id'],ascending=True).index)\n",
    "zzz=[]\n",
    "for i in range(0,11200-1,2):\n",
    "    zzz.append(abs(mmm[i+1]-mmm[i]))\n",
    "zzz=pd.Series(zzz)\n",
    "print(zzz.value_counts())\n",
    "\n",
    "mmm=list(q1.sort_values(by=['id'],ascending=True).index)\n",
    "zzz=[]\n",
    "for i in range(0,11200-1,2):\n",
    "    zzz.append([q1.loc[mmm[i+1],'id'],abs(mmm[i+1]-mmm[i])])\n",
    "zzz=pd.DataFrame(zzz,columns=['id','juli'])\n",
    "print(zzz.sort_values(by=['id'],ascending=True))\n",
    "print(zzz[zzz.juli<4500].shape)\n",
    "leak_id=list(zzz[zzz.juli<4500]['id'])\n",
    "#print(pd.DataFrame(zzz['juli'].value_counts()))\n",
    "#guo=pd.DataFrame(zzz['juli'].value_counts())\n",
    "#print(guo[guo.juli<2].shape)\n",
    "#print(q1[q1.id.isin([int(i) for i in cuo_id['id']])].sort_values(by=['id'],ascending=True))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "3FAC03AA94E8498B811D00D68ED87FE1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不在表2和6的用户数 1532\n",
      "count    50200.000000\n",
      "mean         0.151749\n",
      "std          0.296717\n",
      "min          0.001349\n",
      "25%          0.001949\n",
      "50%          0.002431\n",
      "75%          0.104898\n",
      "max          0.998063\n",
      "Name: Pred, dtype: float64\n",
      "4296\n",
      "50200\n",
      "count    1532.000000\n",
      "mean        0.478976\n",
      "std         0.016612\n",
      "min         0.008491\n",
      "25%         0.481293\n",
      "50%         0.481293\n",
      "75%         0.481293\n",
      "max         0.561069\n",
      "Name: Pred, dtype: float64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-a9090459ab85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mres_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPred\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#print(zzz[zzz.id.isin([int(i) for i in res_id])]['juli'].value_counts())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0med\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mres3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'账期'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'账期'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'账期'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mres3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'账期'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'账期'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ed' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 35 s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "w=list(set(test2['id']))\n",
    "w1=list(test['id'])\n",
    "not_in_test2=[i for i in w1 if i not in w]\n",
    "w2=list(set(test6['id']))\n",
    "not_2_6=[i for i in not_in_test2 if i not in w2]\n",
    "print('不在表2和6的用户数',len(not_2_6))\n",
    "#print(len(not_in_test2))\n",
    "#print(feature_importance_df.sort_values(by='importance',ascending=False))\n",
    "#print(test5.shape,test5.nunique())\n",
    "#print(f5(test5)[:100])\n",
    "#print(n5.shape,n5.nunique())\n",
    "#print(q5.shape,q5.nunique())\n",
    "#w=train[['id','label','6是否去过黔东南目标景区',\\\n",
    "#'7是否去过黔东南目标景区']]\n",
    "#w1=test[['id','6是否去过黔东南目标景区',\\\n",
    "#'7是否去过黔东南目标景区']]\n",
    "#w=pd.merge(w,w1,how='left',on=['id'])\n",
    "#w.columns=['id','y','6','7','66','77']\n",
    "#print(w.sort_values(by='77',ascending=True))\n",
    "#print(w1.sort_values(by='id',ascending=False))\n",
    "\n",
    "#print(test[test.id.isin(list(xiu2.id))][['id','6是否去过黔东南目标景区',\\\n",
    "#'7是否去过黔东南目标景区']]['7是否去过黔东南目标景区'].value_counts())\n",
    "\n",
    "sub=test[['id']].astype(str)\n",
    "sub['Pred']=prediction_xgb1\n",
    "#sub=sub[~sub.id.isin([str(i) for i in list(xiu2.id)])].reset_index(drop=True)\n",
    "#uid6=list(te['id'])\n",
    "#print(len(uid6))\n",
    "#print(len([i for i in uid6 if i not in null_id]))\n",
    "#print(len(list(set(test1['id']))))\n",
    "sub['Pred']=sub['Pred'].apply(lambda x:round(x,8))\n",
    "sub.columns=['ID','Pred']\n",
    "#sub.loc[sub.ID.isin([str(i) for i in leak_id]),'Pred']=1.0000#0.0000\n",
    "#sub.loc[sub.ID.isin([str(i) for i in list(idss)]),'Pred']=0.0000\n",
    "sub.to_csv('mysubmission.csv',index=None)\n",
    "print(sub['Pred'].describe())\n",
    "print(len(sub[sub.Pred>0.6]))\n",
    "print(len(sub[sub.Pred>0.0]))\n",
    "print(sub[sub.ID.isin([str(i) for i in not_in_test2])]['Pred'].describe())\n",
    "#print(sub[sub.ID.isin([str(i) for i in null_id if i not in null_id2])]['Pred'].describe())\n",
    "res_id=list(sub[sub.Pred>0.6]['ID'])\n",
    "#print(zzz[zzz.id.isin([int(i) for i in res_id])]['juli'].value_counts())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "1BE534FE3C744E7C99F53F3D271201E7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!wget -nv -O kesci_submit https://www.heywhale.com/kesci_submit&&chmod +x kesci_submit\n",
    "#!wget -O kesci_submit https://www.heywhale.com/kesci_submit&&chmod +x kesci_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "EE3999E933FB4927B6F6D069DD3A0E1D",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kesci Submit Tool 3.0\n",
      "\n",
      "> 已验证Token\n",
      "> 提交文件 mysubmission.csv (1366.88 KiB)\n",
      "> 文件已上传        \n",
      "> 提交完成\n",
      "time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "##提交结果：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CC6C7D7F085A482B8041940E2A1F376B",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
