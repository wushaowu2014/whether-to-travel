{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C199525CFE7C47DD89EBD4744DCF2794",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzlt\n",
      "201708n  201708q  weather_data_2017\n",
      "201708n1.txt  201708n3.txt  201708n5.txt  201708n7.txt\n",
      "201708n2.txt  201708n4.txt  201708n6.txt\n",
      "201708q1.txt  201708q3.txt  201708q5.txt  201708q7.txt\n",
      "201708q2.txt  201708q4.txt  201708q6.txt\n",
      "201808\tweather_data_2018\n",
      "2018_1.txt  2018_3.txt\t2018_5.txt  2018_7.txt\n",
      "2018_2.txt  2018_4.txt\t2018_6.txt\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录\n",
    "!ls /home/kesci/input/\n",
    "!ls /home/kesci/input/gzlt/train_set/\n",
    "!ls /home/kesci/input/gzlt/train_set/201708n/\n",
    "!ls /home/kesci/input/gzlt/train_set/201708q/\n",
    "!ls /home/kesci/input/gzlt/test_set/\n",
    "!ls /home/kesci/input/gzlt/test_set/201808/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D2025EC9479C47D782144AE47CC47FEC",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kesci_submit  lost+found  mysubmission.csv\r\n"
     ]
    }
   ],
   "source": [
    "# 查看个人持久化工作区文件\n",
    "!ls /home/kesci/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "68095E5A8A754AC081ECCD7488E17B9A",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 查看当前kernerl下的package\n",
    "#!pip list --format=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7DEE463228B243239D2C621748893D7E",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A6E903D21C474AC2AE9BB00DDF7A2D45",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "B2E29DB0063A41C4BD037C05D3A1D73B",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.3 ms\n"
     ]
    }
   ],
   "source": [
    "def r_d(f):\n",
    "    n1= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s1.txt\"%(f,f), sep='\\t',header=None)\n",
    "    n1.columns=['账期','id','出账收入','labe']\n",
    "    n2= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s2.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n2.shape,'************\\n',n2[:5])\n",
    "    n2.columns=['id','手机品牌','终端型号','首次使用时间','末次使用时间','labe']\n",
    "    n3= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s3.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n3.shape,'************\\n',n3[:5])\n",
    "    n3.columns=['账期','id','联络圈规模','是否出省','是否出境','labe']\n",
    "    n4= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s4.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n4.shape,'************\\n',n4[:5])\n",
    "    n4.columns=['账期','id','漫出省份','labe']\n",
    "    n5= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s5.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n5.shape,'************\\n',n5[:5])\n",
    "    n5.columns=['账期','id','是否去过黔东南目标景区','labe']\n",
    "    n6= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s6.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n6.shape,'************\\n',n6[:5])\n",
    "    n6.columns=['日期','时段','id','经度','纬度','labe']\n",
    "    n7= pd.read_csv(\"/home/kesci/input/gzlt/train_set/201708%s/201708%s7.txt\"%(f,f), sep='\\t',header=None)\n",
    "    #print(n7.shape,'************\\n',n7[:5])\n",
    "    n7.columns=['账期','id','APP名称','流量','labe']\n",
    "    uid=pd.DataFrame(list(set(list(n1['id'])+list(n2['id'])+list(n3['id'])+list(n4['id'])+\\\n",
    "        list(n5['id'])+list(n6['id'])+list(n7['id']))),columns=['id'])\n",
    "    return n1,n2,n3,n4,n5,n6,n7,uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BA0967817D3240E28960FEA32F76655A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50200, 1)\n",
      "time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "test1= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_1.txt\", sep='\\t',header=None)\n",
    "#print(test1.shape,'************\\n',test1[:5])\n",
    "test1.columns=['账期','id','出账收入']\n",
    "test2= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_2.txt\", sep='\\t',header=None)\n",
    "#print(test2.shape,'************\\n',test2[:5])\n",
    "test2.columns=['id','手机品牌','终端型号','首次使用时间','末次使用时间']\n",
    "test3= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_3.txt\", sep='\\t',header=None)\n",
    "#print(test3.shape,'************\\n',test3[:5])\n",
    "test3.columns=['账期','id','联络圈规模','是否出省','是否出境']\n",
    "test4= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_4.txt\", sep='\\t',header=None)\n",
    "#print(test4.shape,'************\\n',test4[:5])\n",
    "test4.columns=['账期','id','漫出省份']\n",
    "test5= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_5.txt\", sep='\\t',header=None)\n",
    "#print(test5.shape,'************\\n',test5[:5])\n",
    "test5.columns=['账期','id','是否去过黔东南目标景区']\n",
    "test6= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_6.txt\", sep='\\t',header=None)\n",
    "#print(test6.shape,'************\\n',test6[:5])\n",
    "test6.columns=['日期','时段','id','经度','纬度']\n",
    "test7= pd.read_csv(\"/home/kesci/input/gzlt/test_set/201808/2018_7.txt\", sep='\\t',header=None)\n",
    "#print(test7.shape,'************\\n',test7[:5])\n",
    "test7.columns=['账期','id','APP名称','流量']\n",
    "test_uid=pd.DataFrame(list(set(list(test1['id'])+list(test2['id'])+list(test3['id'])+list(test4['id'])+\\\n",
    "        list(test5['id'])+list(test6['id'])+list(test7['id']))),columns=['id'])\n",
    "print(test_uid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "97BCA18B341445B68C7748A66E54921C",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.64 ms\n"
     ]
    }
   ],
   "source": [
    "def f1(d):\n",
    "    '''每月的出账收入'''\n",
    "    d=d.groupby(['id','账期'],as_index=False)['出账收入'].agg({'出账收入':'mean'})\n",
    "    riqi=list(set(d['账期']))\n",
    "    nd=pd.DataFrame(list(set(d['id'])),columns=['id'])\n",
    "    for i in riqi:\n",
    "        m=d[d['账期']==i][['id','出账收入']]\n",
    "        m.columns=['id',str(i)+'出账收入']\n",
    "        nd=pd.merge(nd,m,how='left',on=['id'])\n",
    "    del riqi,m,d\n",
    "    return nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "35D844404068403C854A81A6F6760954",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139903\n",
      "time: 41.4 s\n"
     ]
    }
   ],
   "source": [
    "n1,n2,n3,n4,n5,n6,n7,n_uid=r_d(f='n')\n",
    "q1,q2,q3,q4,q5,q6,q7,q_uid=r_d('q')\n",
    "print(len(n_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "05A35B2813BE4A248DBF76B4E65D8006",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186800, 6) \n",
      "******************\n",
      " 账期           2\n",
      "id       93400\n",
      "联络圈规模      533\n",
      "是否出省         2\n",
      "是否出境         2\n",
      "labe         1\n",
      "dtype: int64\n",
      "           账期                id  联络圈规模  是否出省  是否出境  labe\n",
      "11199  201706  1030042537821359     83     0     0     1\n",
      "5599   201707  1030042537821359     78     0     0     1\n",
      "5598   201707  1030042459517639     74     0     0     1\n",
      "11198  201706  1030042459517639     66     0     0     1\n",
      "5597   201707  1030042459513741     79     0     0     1\n",
      "11197  201706  1030042459513741     71     0     0     1\n",
      "5596   201707  1030042459510223     69     1     0     1\n",
      "11196  201706  1030042459510223     86     1     0     1\n",
      "11195  201706  1030042459508623      8     0     0     1\n",
      "5595   201707  1030042459508623     15     0     0     1\n",
      "time: 27 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def f2(d):\n",
    "    d=d.fillna(-99)\n",
    "    #nd=pd.DataFrame(list(set(d['id'])),columns=['id'])\n",
    "    m=d.groupby(['id'],as_index=False)['id'].agg({'id_count_2':'count'})\n",
    "    m1=d[d['手机品牌']==-99].groupby(['id'],as_index=False)['手机品牌'].agg({'手机品牌_2':'count'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id']).fillna(0)\n",
    "    m['手机品牌缺失率_2']=m['手机品牌_2']/m['id_count_2']\n",
    "    \n",
    "    m1=d[d['手机品牌']==-99].groupby(['id'],as_index=False)['手机品牌'].agg({'手机品牌nunique_2':'nunique'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    \n",
    "    m1=d[d['终端型号']==-99].groupby(['id'],as_index=False)['终端型号'].agg({'终端型号nunique_2':'nunique'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    \n",
    "    d['末次使用时间']=d['末次使用时间'].apply(lambda x:\\\n",
    "    int(time.mktime(time.strptime(str(int(x)), \"%Y%m%d%H%M%S\"))) if int(x)>0 else -99)\n",
    "    d['首次使用时间']=d['首次使用时间'].apply(lambda x:\\\n",
    "    int(time.mktime(time.strptime(str(int(x)), \"%Y%m%d%H%M%S\"))) if int(x)>0 else -99)\n",
    "    d['末-首次使用时间']=(d['末次使用时间']-d['首次使用时间'])/3600\n",
    "    \n",
    "    m1=d.groupby(['id'],as_index=False)['末-首次使用时间'].agg({'末-首次使用时间mean':'mean',\\\n",
    "        '末-首次使用时间max':'max',\\\n",
    "        '末-首次使用时间min':'min',\\\n",
    "        '末-首次使用时间std':'std',\\\n",
    "         })\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    del m1\n",
    "    ##加小时，周期特征：\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5D753510E808446C81834728C9A15862",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.77 ms\n"
     ]
    }
   ],
   "source": [
    "def f3(d):\n",
    "    m=pd.DataFrame(list(set(d['id'])),columns=['id'])\n",
    "    m1=d[d['联络圈规模']!=-1].groupby(['id'],as_index=False)['联络圈规模'].agg({'联络圈规模_3':'mean'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id']).fillna(-1)\n",
    "    \n",
    "    m1=d.groupby(['id','账期'],as_index=False)['是否出省'].agg({'是否出省':'mean'})\n",
    "    for i in list(set(m1['账期'])):\n",
    "        mm=m1[m1['账期']==i][['id','是否出省']]\n",
    "        mm.columns=['id',str(i)+'是否出省']\n",
    "        m=pd.merge(m,mm,how='left',on=['id'])\n",
    "    m1=d.groupby(['id','账期'],as_index=False)['是否出境'].agg({'是否出境':'mean'})\n",
    "    for i in list(set(m1['账期'])):\n",
    "        mm=m1[m1['账期']==i][['id','是否出境']]\n",
    "        mm.columns=['id',str(i)+'是否出境']\n",
    "        m=pd.merge(m,mm,how='left',on=['id'])\n",
    "    del m1\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3DEAD94B13EC4822BD0E1A12F5CEB936",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36499, 4) \n",
      "******************\n",
      " 账期          2\n",
      "id      12958\n",
      "漫出省份       31\n",
      "labe        1\n",
      "dtype: int64\n",
      "          账期                id 漫出省份  labe\n",
      "7288  201706  1030042459510223   广东     1\n",
      "7283  201706  1030042459510223   广东     1\n",
      "3925  201707  1030042459510223   上海     1\n",
      "3927  201707  1030042459510223   浙江     1\n",
      "7282  201706  1030042459510223   山东     1\n",
      "3926  201707  1030042459510223   江苏     1\n",
      "7284  201706  1030042459510223   广东     1\n",
      "7285  201706  1030042459510223   山东     1\n",
      "7287  201706  1030042459510223   广东     1\n",
      "7286  201706  1030042459510223   广东     1\n",
      "[nan, '江苏', '河南', '甘肃', '天津', '上海', '山西', '江西', '浙江', '辽宁', '青海', '广西', '云南', '河北', '陕西', '安徽', '湖北', '贵州', '宁夏', '重庆', '四川', '北京', '广东', '吉林', '山东', '福建', '新疆', '海南', '黑龙江', '湖南', '西藏', '内蒙古']\n",
      "(400, 4)\n",
      "time: 29.1 ms\n"
     ]
    }
   ],
   "source": [
    "def f4(d):\n",
    "    m=d.groupby(['id'],as_index=False)['漫出省份'].agg({'漫出省份count_4':'count',\\\n",
    "       '漫出省份nunique_4':'nunique'\n",
    "    })\n",
    "    m1=d[d['漫出省份'].isnull()].groupby(['id'],as_index=False)['漫出省份'].agg({'漫出省份缺失_4':'count'})\n",
    "    m=pd.merge(m,m1,how='left',on=['id']).fillna(0)\n",
    "    m['漫出省份缺失率']=m['漫出省份缺失_4']/m['漫出省份count_4']\n",
    "    \n",
    "    d=d.fillna(-1)\n",
    "    zz=[]\n",
    "    for i in m['id']:\n",
    "        z={}\n",
    "        m1=d[d['id']==i]['漫出省份']\n",
    "        for j in m1:\n",
    "            z[j]=z.get(j,0)+1\n",
    "        zz.append(z)\n",
    "    zz=pd.DataFrame(zz)\n",
    "    m=pd.concat([m,zz],axis=1)\n",
    "    del m1,z,zz,\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CDB241C25B6F4F3082CB04F0D49586FE",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11200, 4) \n",
      "******************\n",
      " 账期                2\n",
      "id             5600\n",
      "是否去过黔东南目标景区       2\n",
      "labe              1\n",
      "dtype: int64\n",
      "time: 5.68 ms\n"
     ]
    }
   ],
   "source": [
    "def f5(d):\n",
    "    m=pd.DataFrame(list(set(d['id'])),columns=['id'])\n",
    "    for i in list(set(d['账期'])):\n",
    "        m1=d[d['账期']==i][['id','是否去过黔东南目标景区']]\n",
    "        m1.columns=['id',str(i)+'是否去过黔东南目标景区']\n",
    "        m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    del m1\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "69D552E7AC344AA5843BC82A09B398AE",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 日期    时段              id          经度        纬度  labe\n",
      "1836418  2017-06-01  18.0  10000000050393  106.712090  26.58831     1\n",
      "2230035  2017-06-01   2.0  10000000050393  106.712090  26.58831     1\n",
      "2526340  2017-06-01  14.0  10000000050393  106.724756  26.56594     1\n",
      "2756403  2017-06-01  20.0  10000000050393  106.709940  26.58616     1\n",
      "2822199  2017-06-01  16.0  10000000050393  106.712090  26.58831     1\n",
      "1705047  2017-06-02  16.0  10000000050393  106.724756  26.56594     1\n",
      "2032962  2017-06-02  14.0  10000000050393  106.724756  26.56594     1\n",
      "2065723  2017-06-02   5.0  10000000050393  106.712090  26.58831     1\n",
      "2131401  2017-06-02  22.0  10000000050393  106.713790  26.58892     1\n",
      "2263032  2017-06-02  12.0  10000000050393  106.724756  26.56594     1\n",
      "(2852871, 6) \n",
      "******************\n",
      " 日期         62\n",
      "时段         13\n",
      "id       5575\n",
      "经度      23355\n",
      "纬度      23226\n",
      "labe        1\n",
      "dtype: int64\n",
      "time: 3.19 s\n"
     ]
    }
   ],
   "source": [
    "def f6(d):\n",
    "    d['基站']=d['经度'].astype(str)+'_'+d['纬度'].astype(str)\n",
    "    m=d.groupby(['id'],as_index=False)['基站'].agg({'基站count_6':'count',\\\n",
    "       '基站nunique_6':'nunique'\n",
    "    })\n",
    "    return m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8D9850C2B0324BBEA6352600E2F3C393",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173117, 5) \n",
      "******************\n",
      " 账期           2\n",
      "id        4717\n",
      "APP名称      762\n",
      "流量       11485\n",
      "labe         1\n",
      "dtype: int64\n",
      "            账期              id      APP名称     流量  labe\n",
      "11537   201707  10000000050393       腾讯新闻   0.00     1\n",
      "133735  201706  10000000050393        沃音乐   0.01     1\n",
      "161850  201706  10000000050393       腾讯视频   0.01     1\n",
      "65921   201707  10000000050393       高德地图   0.01     1\n",
      "17601   201707  10000000050393     华为应用市场   0.13     1\n",
      "145197  201706  10000000050393         微信  11.63     1\n",
      "929     201707  10000000050393       小米商城   0.01     1\n",
      "27760   201707  10000000050393         QQ   0.64     1\n",
      "94381   201706  10000000050393       小米商城   0.01     1\n",
      "60285   201707  10000000050393        唯品会   2.70     1\n",
      "121992  201706  10000000050393         QQ   0.19     1\n",
      "147207  201706  10000000050393       携程旅行   1.49     1\n",
      "106817  201706  10000000050393        支付宝   0.02     1\n",
      "65069   201707  10000000050393      爱奇艺视频   0.00     1\n",
      "25769   201707  10000000050393       神州专车   0.20     1\n",
      "155255  201706  10000000050393     华为应用市场   0.07     1\n",
      "20764   201707  10000000050393       百度地图   0.73     1\n",
      "29281   201707  10000000050393      开心消消乐   0.00     1\n",
      "68209   201707  10000000050393       新浪微博   0.00     1\n",
      "53357   201707  10000000050393        支付宝   0.10     1\n",
      "89864   201707  10000000050393       携程旅行   2.38     1\n",
      "109484  201706  10000000050393       高德地图   0.16     1\n",
      "7       201707  10000000050393     腾讯手机管家   0.00     1\n",
      "60673   201707  10000000050393         微信  16.97     1\n",
      "96656   201706  10000000056337      搜狗输入法   0.00     1\n",
      "134670  201706  10000000056337  中国联通手机营业厅   0.50     1\n",
      "73188   201707  10000000056337         迅雷   0.15     1\n",
      "1286    201707  10000000056337         微信  74.74     1\n",
      "64149   201707  10000000056337      搜狗输入法   0.00     1\n",
      "79077   201707  10000000056337       酷狗音乐   0.00     1\n",
      "...        ...             ...        ...    ...   ...\n",
      "668     201707  10000000065245         微信  12.21     1\n",
      "144095  201706  10000000065245       今日头条   0.02     1\n",
      "35549   201707  10000000065245       腾讯视频   0.13     1\n",
      "89696   201707  10000000065245      网易云音乐   0.09     1\n",
      "69804   201707  10000000065245      去哪儿旅行   0.56     1\n",
      "65164   201707  10000000065245   爱奇艺PPS影音   0.00     1\n",
      "16220   201707  10000000065245      爱奇艺视频   0.01     1\n",
      "155667  201706  10000000065245         微信   7.08     1\n",
      "74218   201707  10000000065245        支付宝   0.01     1\n",
      "86804   201707  10000000065245       腾讯新闻   0.02     1\n",
      "162768  201706  10000000065245       QQ音乐   0.00     1\n",
      "74334   201707  10000000065245   OPPO软件商店   0.00     1\n",
      "25159   201707  10000000065245       QQ音乐   0.04     1\n",
      "14030   201707  10000000065245         QQ   0.07     1\n",
      "107142  201706  10000000065245      去哪儿旅行   0.80     1\n",
      "101808  201706  10000000110765       网易新闻   0.00     1\n",
      "121080  201706  10000000110765      爱奇艺视频  12.48     1\n",
      "117813  201706  10000000110765      WO+视频   2.77     1\n",
      "6232    201707  10000000110765       手机百度   1.30     1\n",
      "121909  201706  10000000110765         闲鱼   0.03     1\n",
      "129108  201706  10000000110765   AppStore   1.76     1\n",
      "112179  201706  10000000110765      搜狗输入法   0.00     1\n",
      "130326  201706  10000000110765       百度文库   0.00     1\n",
      "124034  201706  10000000110765       墨迹天气   0.13     1\n",
      "69331   201707  10000000110765         闲鱼   0.12     1\n",
      "48239   201707  10000000110765   OPPO软件商店   0.01     1\n",
      "52433   201707  10000000110765       酷狗音乐   0.06     1\n",
      "166873  201706  10000000110765       腾讯视频   0.01     1\n",
      "124326  201706  10000000110765       携程旅行   0.01     1\n",
      "101497  201706  10000000110765       百度贴吧   0.00     1\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "time: 115 ms\n"
     ]
    }
   ],
   "source": [
    "def f7(d):\n",
    "    m=d.groupby(['id'],as_index=False)['APP名称'].agg({'APP名称count_7':'count',\\\n",
    "       'APP名称nunique_7':'nunique'\n",
    "    })\n",
    "    m1=d.groupby(['id'],as_index=False)['流量'].agg({'流量mean_7':'mean',\\\n",
    "       '流量sum_7':'sum',\\\n",
    "       '流量max_7':'max',\\\n",
    "       '流量min_7':'min',\\\n",
    "       '流量std_7':'std',\\\n",
    "    })\n",
    "    m=pd.merge(m,m1,how='left',on=['id'])\n",
    "    del m1\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "D200A9C19224480C82EBFE962902DA95",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5600, 64) (139903, 65) (50200, 64)\n",
      "time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "q_uid['label']=1\n",
    "n_uid['label']=0\n",
    "q_train=pd.merge(q_uid,f1(q1),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f2(q2),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f3(q3),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f4(q4),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f5(q5),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f6(q6),how='left',on=['id'])\n",
    "q_train=pd.merge(q_train,f7(q7),how='left',on=['id'])\n",
    "\n",
    "n_train=pd.merge(n_uid,f1(n1),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f2(n2),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f3(n3),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f4(n4),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f5(n5),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f6(n6),how='left',on=['id'])\n",
    "n_train=pd.merge(n_train,f7(n7),how='left',on=['id'])\n",
    "\n",
    "test=pd.merge(test_uid,f1(test1),how='left',on=['id'])\n",
    "test=pd.merge(test,f2(test2),how='left',on=['id'])\n",
    "test=pd.merge(test,f3(test3),how='left',on=['id'])\n",
    "test=pd.merge(test,f4(test4),how='left',on=['id'])\n",
    "test=pd.merge(test,f5(test5),how='left',on=['id'])\n",
    "test=pd.merge(test,f6(test6),how='left',on=['id'])\n",
    "test=pd.merge(test,f7(test7),how='left',on=['id'])\n",
    "\n",
    "print(q_train.shape,n_train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "D9DC88C54D8447BA8F8F276A4CE249BF",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    139903\n",
      "1      5600\n",
      "Name: label, dtype: int64\n",
      "6015600067 1030042537821359\n",
      "99000\n",
      "10000000005691\n",
      "10000000050393 1036145391789885\n",
      "相同用户个数： 3519\n",
      "3519\n",
      "相同用户标签分布： 1    3262\n",
      "0     257\n",
      "Name: label, dtype: int64\n",
      "time: 4.67 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train=pd.concat([q_train,n_train],axis=0).reset_index(drop=True)\n",
    "label=train['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7D243194F85141FF9FBA2AD8E918547A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "def xgb_model(new_train,y,new_test):\n",
    "  '''定义模型'''\n",
    "  xgb_params = {'booster': 'gbtree',\n",
    "          'eta': 0.01, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "          'objective':'binary:logistic',\n",
    "          'eval_metric': 'auc',\n",
    "          'silent': True,\n",
    "          }\n",
    "  #skf=StratifiedKFold(y,n_folds=5,shuffle=True,random_state=2018)\n",
    "  skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "  oof_xgb=np.zeros(len(new_train))\n",
    "  prediction_xgb=np.zeros(len(new_test))\n",
    "  for i,(tr,va) in enumerate(skf.split(new_train,y)):\n",
    "    print('fold:',i+1,'training')\n",
    "    dtrain = xgb.DMatrix(new_train[tr],y[tr])\n",
    "    dvalid = xgb.DMatrix(new_train[va],y[va])\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid_data')]\n",
    "    bst = xgb.train(dtrain=dtrain, num_boost_round=30000, evals=watchlist, early_stopping_rounds=200, verbose_eval=200, params=xgb_params)\n",
    "    oof_xgb[va] += bst.predict(xgb.DMatrix(new_train[va]), ntree_limit=bst.best_ntree_limit)\n",
    "    prediction_xgb += bst.predict(xgb.DMatrix(new_test), ntree_limit=bst.best_ntree_limit)\n",
    "  print('the roc_auc_score for train:',roc_auc_score(y,oof_xgb))\n",
    "  prediction_xgb/=5\n",
    "  return oof_xgb,prediction_xgb\n",
    "def lgb_model(new_train,y,new_test):\n",
    "    params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'num_leaves': 1000,\n",
    "    'verbose': -1,\n",
    "    'max_depth': -1,\n",
    "  #  'reg_alpha':2.2,\n",
    "  #  'reg_lambda':1.4,\n",
    "    'seed':42,\n",
    "    }\n",
    "    #skf=StratifiedKFold(y,n_folds=5,shuffle=True,random_state=2018)\n",
    "    skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "    oof_lgb=np.zeros(len(new_train))\n",
    "    prediction_lgb=np.zeros(len(new_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i,(tr,va) in enumerate(skf.split(new_train,y)):\n",
    "        print('fold:',i+1,'training')\n",
    "        dtrain = lgb.Dataset(new_train.loc[tr],y[tr])\n",
    "        dvalid = lgb.Dataset(new_train.loc[va],y[va],reference=dtrain)\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=30000, valid_sets=dvalid, verbose_eval=400,early_stopping_rounds=200)\n",
    "        oof_lgb[va] += bst.predict(new_train.loc[va], num_iteration=bst.best_iteration)\n",
    "        prediction_lgb += bst.predict(new_test, num_iteration=bst.best_iteration)\n",
    "    \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = list(new_train.columns)\n",
    "        fold_importance_df[\"importance\"] = bst.feature_importance(importance_type='split', iteration=bst.best_iteration)\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    \n",
    "    print('the roc_auc_score for train:',roc_auc_score(y,oof_lgb))\n",
    "    prediction_lgb/=5\n",
    "    return oof_lgb,prediction_lgb,feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "F23C982AD503407C9AAF0B77D0618EAB",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\tvalid_0's auc: 0.988638\n",
      "[800]\tvalid_0's auc: 0.989182\n",
      "Early stopping, best iteration is:\n",
      "[871]\tvalid_0's auc: 0.989271\n",
      "fold: 2 training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\tvalid_0's auc: 0.985631\n",
      "[800]\tvalid_0's auc: 0.9863\n",
      "Early stopping, best iteration is:\n",
      "[673]\tvalid_0's auc: 0.986347\n",
      "fold: 3 training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\tvalid_0's auc: 0.987702\n",
      "[800]\tvalid_0's auc: 0.988018\n",
      "Early stopping, best iteration is:\n",
      "[910]\tvalid_0's auc: 0.988059\n",
      "fold: 4 training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\tvalid_0's auc: 0.987967\n",
      "[800]\tvalid_0's auc: 0.988441\n",
      "[1200]\tvalid_0's auc: 0.988492\n",
      "Early stopping, best iteration is:\n",
      "[1070]\tvalid_0's auc: 0.988671\n",
      "fold: 5 training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\tvalid_0's auc: 0.989911\n",
      "[800]\tvalid_0's auc: 0.990455\n",
      "Early stopping, best iteration is:\n",
      "[753]\tvalid_0's auc: 0.990482\n",
      "the roc_auc_score for train: 0.9810172596625623\n",
      "time: 8min 42s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "oof_lgb,prediction_lgb,feature_importance_df=\\\n",
    "lgb_model(train.drop(['id','label'],axis=1),label,test.drop(['id'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1043F574DDE14F42845EA463A6346AB4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 training\n",
      "[0]\ttrain-auc:0.976254\tvalid_data-auc:0.972295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.989506\tvalid_data-auc:0.988515\n",
      "[400]\ttrain-auc:0.990533\tvalid_data-auc:0.988021\n",
      "Stopping. Best iteration:\n",
      "[209]\ttrain-auc:0.989797\tvalid_data-auc:0.988845\n",
      "\n",
      "fold: 2 training\n",
      "[0]\ttrain-auc:0.974975\tvalid_data-auc:0.968626\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.990046\tvalid_data-auc:0.98396\n",
      "[400]\ttrain-auc:0.991233\tvalid_data-auc:0.985781\n",
      "[600]\ttrain-auc:0.992736\tvalid_data-auc:0.987824\n",
      "[800]\ttrain-auc:0.994069\tvalid_data-auc:0.988232\n",
      "[1000]\ttrain-auc:0.99549\tvalid_data-auc:0.988311\n",
      "Stopping. Best iteration:\n",
      "[997]\ttrain-auc:0.995471\tvalid_data-auc:0.988316\n",
      "\n",
      "fold: 3 training\n",
      "[0]\ttrain-auc:0.974694\tvalid_data-auc:0.971352\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.989678\tvalid_data-auc:0.986985\n",
      "[400]\ttrain-auc:0.991035\tvalid_data-auc:0.988707\n",
      "[600]\ttrain-auc:0.992696\tvalid_data-auc:0.98904\n",
      "Stopping. Best iteration:\n",
      "[496]\ttrain-auc:0.991958\tvalid_data-auc:0.989172\n",
      "\n",
      "fold: 4 training\n",
      "[0]\ttrain-auc:0.970313\tvalid_data-auc:0.970623\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.989272\tvalid_data-auc:0.98645\n",
      "[400]\ttrain-auc:0.991055\tvalid_data-auc:0.98847\n",
      "[600]\ttrain-auc:0.992254\tvalid_data-auc:0.98966\n",
      "[800]\ttrain-auc:0.993575\tvalid_data-auc:0.989758\n",
      "Stopping. Best iteration:\n",
      "[668]\ttrain-auc:0.992834\tvalid_data-auc:0.989844\n",
      "\n",
      "fold: 5 training\n",
      "[0]\ttrain-auc:0.948122\tvalid_data-auc:0.950271\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.988961\tvalid_data-auc:0.989456\n",
      "[400]\ttrain-auc:0.990571\tvalid_data-auc:0.990077\n",
      "[600]\ttrain-auc:0.992425\tvalid_data-auc:0.990451\n",
      "Stopping. Best iteration:\n",
      "[596]\ttrain-auc:0.992411\tvalid_data-auc:0.990464\n",
      "\n",
      "the roc_auc_score for train: 0.9770225754629994\n",
      "time: 7min 12s\n"
     ]
    }
   ],
   "source": [
    "oof_xgb,prediction_xgb=\\\n",
    "xgb_model(np.array(train.drop(['id','label'],axis=1)),label,np.array(test.drop(['id'],axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "E5EC2AFE7EC9472196DC423A7CEF3CCF",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix for lr:\n",
      " [[139880     23]\n",
      " [   605   4995]]\n",
      "classification_report for lr\n",
      ":               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    139903\n",
      "           1       1.00      0.89      0.94      5600\n",
      "\n",
      "    accuracy                           1.00    145503\n",
      "   macro avg       1.00      0.95      0.97    145503\n",
      "weighted avg       1.00      1.00      1.00    145503\n",
      "\n",
      "time: 378 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "r=[1 if i>0.4 else 0 for i in oof_xgb]\n",
    "print('confusion_matrix for lr:\\n', metrics.confusion_matrix(label,r))\n",
    "print('classification_report for lr\\n:', metrics.classification_report(label,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "3FAC03AA94E8498B811D00D68ED87FE1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    46681.000000\n",
      "mean         0.479861\n",
      "std          0.251511\n",
      "min          0.015098\n",
      "25%          0.381349\n",
      "50%          0.579835\n",
      "75%          0.593643\n",
      "max          0.957090\n",
      "Name: Pred, dtype: float64\n",
      "11271\n",
      "time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "sub=test[['id']].astype(str)\n",
    "sub['Pred']=prediction_xgb\n",
    "'''\n",
    "for i in range(len(xiu2)):\n",
    "    if int(xiu2.loc[i,'label'])==1:\n",
    "        sub.loc[sub.id==str(xiu2.loc[i,'id']),'Pred']=0#xiu2.loc[i,'label']\n",
    "    else:\n",
    "        sub.loc[sub.id==str(xiu2.loc[i,'id']),'Pred']=1\n",
    "'''\n",
    "sub=sub[~sub.id.isin([str(i) for i in list(xiu2.id)])].reset_index(drop=True)\n",
    "\n",
    "sub['Pred']=sub['Pred'].apply(lambda x:round(x,6))\n",
    "sub.columns=['ID','Pred']\n",
    "sub.to_csv('mysubmission.csv',index=None)\n",
    "print(sub['Pred'].describe())\n",
    "print(len(sub[sub.Pred>0.6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "1BE534FE3C744E7C99F53F3D271201E7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /opt/conda/lib/libcrypto.so.1.0.0: no version information available (required by wget)\n",
      "wget: /opt/conda/lib/libssl.so.1.0.0: no version information available (required by wget)\n",
      "wget: /opt/conda/lib/libssl.so.1.0.0: no version information available (required by wget)\n",
      "2019-06-18 06:19:59 URL:https://www.heywhale.com/kesci_submit [7842088/7842088] -> \"kesci_submit\" [1]\n",
      "time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "#!wget -nv -O kesci_submit https://www.heywhale.com/kesci_submit&&chmod +x kesci_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EE3999E933FB4927B6F6D069DD3A0E1D",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kesci Submit Tool\n",
      "Result File: mysubmission.csv (1.15 MiB)\n",
      "Uploaded.       \n",
      "====================\n",
      "Submit Success.\n",
      "{\"Stage\":0,\"Status\":0,\"ShownInHistory\":true,\"IsAucResult\":true,\"Selected\":false,\"_id\":\"5d09ab5e921a91002b8dd863\",\"Competition\":\"5be92233954d6e001063649a\",\"Team\":\"5d0476d0902bad002c295284\",\"UploadDate\":\"2019-06-19T03:26:22.404Z\",\"Final\":true,\"Response\":\"\",\"SubmissionResults\":[],\"IP\":\"52.82.22.223\",\"FingerPrint\":\"\",\"UserAgent\":\"Go-http-client/1.1\",\"ResultFileName\":\"1560914781999c4a7f9.csv\",\"ResultFileRealName\":\"mysubmission.csv\",\"ResultFileSize\":0,\"ReviewInfos\":[],\"__v\":0}\n",
      "\n",
      "time: 1.54 s\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CC6C7D7F085A482B8041940E2A1F376B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
